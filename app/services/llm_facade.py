"""
LLM Facade Layer with Intelligent Model Routing

ë¹„ìš© ìµœì í™”ë¥¼ ìœ„í•œ Bedrock ëª¨ë¸ ìë™ ë¼ìš°íŒ… ì‹œìŠ¤í…œ
- ì¿¼ë¦¬ ë³µì¡ë„ ê¸°ë°˜ ëª¨ë¸ ì„ íƒ (Haiku vs Sonnet)
- CloudWatch ë©”íŠ¸ë¦­ ì „ì†¡ìœ¼ë¡œ ë¹„ìš© ì¶”ì 
- ìë™ í´ë°± ë©”ì»¤ë‹ˆì¦˜
"""

import boto3
import time
import json
from enum import Enum
from dataclasses import dataclass
from typing import Optional, Dict, List
from datetime import datetime


class BedrockModel(Enum):
    """ì‚¬ìš© ê°€ëŠ¥í•œ Bedrock ëª¨ë¸"""
    HAIKU = "anthropic.claude-3-haiku-20240307-v1:0"
    HAIKU_35 = "anthropic.claude-3-5-haiku-20241022-v1:0"  # ì£¼ë ¥ ëª¨ë¸
    SONNET_35 = "anthropic.claude-3-5-sonnet-20241022-v2:0"  # ë³µì¡í•œ ì¿¼ë¦¬ìš©


@dataclass
class ModelPricing:
    """ëª¨ë¸ë³„ í† í° ê°€ê²© (USD per 1K tokens)"""
    input_per_1k: float
    output_per_1k: float


# 2025ë…„ Bedrock ê°€ê²©í‘œ
PRICING = {
    BedrockModel.HAIKU: ModelPricing(0.00025, 0.00125),
    BedrockModel.HAIKU_35: ModelPricing(0.0008, 0.004),
    BedrockModel.SONNET_35: ModelPricing(0.003, 0.015),
}


class QueryComplexityAnalyzer:
    """ì¿¼ë¦¬ ë³µì¡ë„ ë¶„ì„ ë° ì ì ˆí•œ ëª¨ë¸ ì„ íƒ"""

    @staticmethod
    def calculate_complexity(
        query: str,
        context_chunks: List[str],
        user_tier: str = "free",
        conversation_history: Optional[List[Dict]] = None
    ) -> float:
        """
        ì¿¼ë¦¬ ë³µì¡ë„ ì ìˆ˜ ê³„ì‚° (0.0 - 1.0)

        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            context_chunks: RAG ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ ì²­í¬ë“¤
            user_tier: ì‚¬ìš©ì ë“±ê¸‰ (free/premium)
            conversation_history: ëŒ€í™” íˆìŠ¤í† ë¦¬ (ì„ íƒ)

        Returns:
            0.0-0.3: ë‹¨ìˆœ (Haiku 3.5)
            0.3-0.7: ë³´í†µ (Haiku 3.5)
            0.7-1.0: ë³µì¡ (Sonnet 3.5)
        """
        score = 0.0

        # Factor 1: ì¿¼ë¦¬ ê¸¸ì´ (ê¸´ ì¿¼ë¦¬ = ë³µì¡í•œ ì§ˆë¬¸)
        query_tokens = len(query.split())
        if query_tokens > 100:
            score += 0.3
        elif query_tokens > 50:
            score += 0.2
        elif query_tokens > 20:
            score += 0.1

        # Factor 2: ì»¨í…ìŠ¤íŠ¸ í¬ê¸° (ë§ì€ ì»¨í…ìŠ¤íŠ¸ = ë³µì¡í•œ ì¶”ë¡  í•„ìš”)
        total_context_tokens = sum(len(chunk.split()) for chunk in context_chunks)
        if total_context_tokens > 3000:
            score += 0.3
        elif total_context_tokens > 1500:
            score += 0.2
        elif total_context_tokens > 500:
            score += 0.1

        # Factor 3: ë³µì¡í•œ ì¿¼ë¦¬ íƒ€ì… ê°ì§€
        complex_keywords = [
            'analyze', 'compare', 'explain why', 'reasoning',
            'complex', 'detailed analysis', 'pros and cons',
            'evaluate', 'assess', 'critique', 'synthesize',
            'ë¶„ì„', 'ë¹„êµ', 'ì´ìœ ', 'ì¥ë‹¨ì ', 'í‰ê°€', 'ì¢…í•©'
        ]
        if any(keyword in query.lower() for keyword in complex_keywords):
            score += 0.2

        # Factor 4: ë‹¤ë‹¨ê³„ ì¶”ë¡  ì§€ì‹œì–´ ê°ì§€
        multi_step_indicators = [
            'first', 'then', 'finally', 'step by step',
            'ì²«ì§¸', 'ë‘˜ì§¸', 'ë§ˆì§€ë§‰ìœ¼ë¡œ', 'ë‹¨ê³„ë³„'
        ]
        if any(indicator in query.lower() for indicator in multi_step_indicators):
            score += 0.15

        # Factor 5: ëŒ€í™” íˆìŠ¤í† ë¦¬ ê¸¸ì´ (ê¸´ ëŒ€í™” = ë³µì¡í•œ ë§¥ë½)
        if conversation_history and len(conversation_history) > 5:
            score += 0.1

        # Factor 6: ì‚¬ìš©ì ë“±ê¸‰ (í”„ë¦¬ë¯¸ì—„ ì‚¬ìš©ìëŠ” ë” ë‚˜ì€ ëª¨ë¸ í¸í–¥)
        if user_tier == "premium":
            score += 0.1

        return min(1.0, score)

    @staticmethod
    def select_model(
        complexity: float,
        user_tier: str = "free",
        force_model: Optional[BedrockModel] = None
    ) -> BedrockModel:
        """ë³µì¡ë„ ê¸°ë°˜ ëª¨ë¸ ì„ íƒ"""

        if force_model:
            return force_model

        # Premium ì‚¬ìš©ì: Haiku 3.5 ë˜ëŠ” Sonnet
        if user_tier == "premium":
            return BedrockModel.SONNET_35 if complexity > 0.6 else BedrockModel.HAIKU_35

        # Free tier: ë¹„ìš© ìµœì í™”
        if complexity < 0.3:
            return BedrockModel.HAIKU_35  # ë‹¨ìˆœ ì¿¼ë¦¬
        elif complexity < 0.7:
            return BedrockModel.HAIKU_35  # ë³´í†µ ì¿¼ë¦¬
        else:
            return BedrockModel.SONNET_35  # ë³µì¡í•œ ì¿¼ë¦¬ë§Œ


class LLMFacade:
    """Bedrock LLM í†µí•© ì¸í„°í˜ì´ìŠ¤ (ì§€ëŠ¥í˜• ë¼ìš°íŒ…)"""

    def __init__(
        self,
        region_name: str = "us-east-1",
        enable_cloudwatch: bool = True
    ):
        """
        Args:
            region_name: AWS ë¦¬ì „
            enable_cloudwatch: CloudWatch ë©”íŠ¸ë¦­ ì „ì†¡ ì—¬ë¶€
        """
        self.bedrock = boto3.client('bedrock-runtime', region_name=region_name)
        self.cloudwatch = boto3.client('cloudwatch', region_name=region_name) if enable_cloudwatch else None
        self.analyzer = QueryComplexityAnalyzer()
        self.enable_metrics = enable_cloudwatch

    def invoke(
        self,
        query: str,
        context_chunks: List[str],
        user_id: str,
        user_tier: str = "free",
        conversation_history: Optional[List[Dict]] = None,
        force_model: Optional[BedrockModel] = None,
        max_tokens: int = 2048,
        temperature: float = 0.7
    ) -> Dict:
        """
        LLM í˜¸ì¶œ (ì§€ëŠ¥í˜• ëª¨ë¸ ì„ íƒ)

        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            context_chunks: RAG ê²€ìƒ‰ ê²°ê³¼
            user_id: ì‚¬ìš©ì ID
            user_tier: ì‚¬ìš©ì ë“±ê¸‰
            conversation_history: ëŒ€í™” íˆìŠ¤í† ë¦¬
            force_model: ê°•ì œ ëª¨ë¸ ì„ íƒ (í…ŒìŠ¤íŠ¸ìš©)
            max_tokens: ìµœëŒ€ ìƒì„± í† í°
            temperature: ìƒì„± ì˜¨ë„

        Returns:
            {
                'response': str,  # LLM ì‘ë‹µ
                'model_used': str,  # ì‚¬ìš©ëœ ëª¨ë¸
                'complexity': float,  # ë³µì¡ë„ ì ìˆ˜
                'input_tokens': int,
                'output_tokens': int,
                'cost': float,  # USD
                'latency_ms': int
            }
        """
        start_time = time.time()

        # ëª¨ë¸ ì„ íƒ
        if force_model:
            model = force_model
            complexity = None
        else:
            complexity = self.analyzer.calculate_complexity(
                query, context_chunks, user_tier, conversation_history
            )
            model = self.analyzer.select_model(complexity, user_tier)

        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = self._build_prompt(query, context_chunks, conversation_history)

        # Bedrock í˜¸ì¶œ
        try:
            response = self._invoke_bedrock(
                model=model,
                prompt=prompt,
                max_tokens=max_tokens,
                temperature=temperature
            )

            latency_ms = int((time.time() - start_time) * 1000)

            # ë¹„ìš© ê³„ì‚°
            input_tokens = response['usage']['input_tokens']
            output_tokens = response['usage']['output_tokens']
            cost = self._calculate_cost(model, input_tokens, output_tokens)

            # CloudWatch ë©”íŠ¸ë¦­ ì „ì†¡
            if self.enable_metrics:
                self._publish_metrics(
                    model=model,
                    complexity=complexity,
                    input_tokens=input_tokens,
                    output_tokens=output_tokens,
                    cost=cost,
                    latency_ms=latency_ms,
                    user_tier=user_tier,
                    fallback=False
                )

            return {
                'response': response['content'][0]['text'],
                'model_used': model.name,
                'complexity': complexity,
                'input_tokens': input_tokens,
                'output_tokens': output_tokens,
                'cost': cost,
                'latency_ms': latency_ms,
                'fallback_occurred': False
            }

        except Exception as e:
            # í´ë°±: Sonnet ì‹¤íŒ¨ ì‹œ Haikuë¡œ ì¬ì‹œë„
            if model == BedrockModel.SONNET_35:
                print(f"âš ï¸ Sonnet failed, falling back to Haiku 3.5: {e}")

                fallback_response = self.invoke(
                    query=query,
                    context_chunks=context_chunks,
                    user_id=user_id,
                    user_tier=user_tier,
                    conversation_history=conversation_history,
                    force_model=BedrockModel.HAIKU_35,
                    max_tokens=max_tokens,
                    temperature=temperature
                )

                # í´ë°± ë©”íŠ¸ë¦­ ì „ì†¡
                if self.enable_metrics:
                    self.cloudwatch.put_metric_data(
                        Namespace='RAG/LLM',
                        MetricData=[{
                            'MetricName': 'ModelFallback',
                            'Value': 1,
                            'Unit': 'Count',
                            'Timestamp': datetime.utcnow(),
                            'Dimensions': [
                                {'Name': 'FromModel', 'Value': 'SONNET_35'},
                                {'Name': 'ToModel', 'Value': 'HAIKU_35'}
                            ]
                        }]
                    )

                fallback_response['fallback_occurred'] = True
                return fallback_response
            else:
                raise

    def _invoke_bedrock(
        self,
        model: BedrockModel,
        prompt: str,
        max_tokens: int,
        temperature: float
    ) -> Dict:
        """Bedrock API í˜¸ì¶œ"""

        body = {
            "anthropic_version": "bedrock-2023-05-31",
            "max_tokens": max_tokens,
            "temperature": temperature,
            "messages": [
                {"role": "user", "content": prompt}
            ]
        }

        response = self.bedrock.invoke_model(
            modelId=model.value,
            body=json.dumps(body)
        )

        response_body = json.loads(response['body'].read())
        return response_body

    def _build_prompt(
        self,
        query: str,
        context_chunks: List[str],
        conversation_history: Optional[List[Dict]] = None
    ) -> str:
        """RAG í”„ë¡¬í”„íŠ¸ êµ¬ì„±"""

        # ì»¨í…ìŠ¤íŠ¸ í¬ë§·íŒ…
        context = "\n\n".join(
            f"[ë¬¸ì„œ {i+1}]\n{chunk}"
            for i, chunk in enumerate(context_chunks)
        )

        # ëŒ€í™” íˆìŠ¤í† ë¦¬ í¬í•¨ (ì„ íƒ)
        history_text = ""
        if conversation_history:
            history_items = []
            for msg in conversation_history[-5:]:  # ìµœê·¼ 5ê°œë§Œ
                role = "ì‚¬ìš©ì" if msg['role'] == 'user' else "AI"
                history_items.append(f"{role}: {msg['content']}")
            history_text = f"\n\n<ëŒ€í™” íˆìŠ¤í† ë¦¬>\n{chr(10).join(history_items)}\n</ëŒ€í™” íˆìŠ¤í† ë¦¬>\n"

        prompt = f"""{history_text}
<ë¬¸ì„œ>
{context}
</ë¬¸ì„œ>

<ì§ˆë¬¸>
{query}
</ì§ˆë¬¸>

ìœ„ ë¬¸ì„œë“¤ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.
ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ê³ , ë¬¸ì„œ ê¸°ë°˜ìœ¼ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”."""

        return prompt

    def _calculate_cost(
        self,
        model: BedrockModel,
        input_tokens: int,
        output_tokens: int
    ) -> float:
        """ë¹„ìš© ê³„ì‚° (USD)"""
        pricing = PRICING[model]
        input_cost = (input_tokens / 1000) * pricing.input_per_1k
        output_cost = (output_tokens / 1000) * pricing.output_per_1k
        return input_cost + output_cost

    def _publish_metrics(
        self,
        model: BedrockModel,
        complexity: Optional[float],
        input_tokens: int,
        output_tokens: int,
        cost: float,
        latency_ms: int,
        user_tier: str,
        fallback: bool
    ):
        """CloudWatch ë©”íŠ¸ë¦­ ì „ì†¡"""

        if not self.cloudwatch:
            return

        namespace = 'RAG/LLM'
        timestamp = datetime.utcnow()

        metrics = [
            # í† í° ì‚¬ìš©ëŸ‰
            {
                'MetricName': 'InputTokens',
                'Value': input_tokens,
                'Unit': 'Count',
                'Timestamp': timestamp,
                'Dimensions': [
                    {'Name': 'Model', 'Value': model.name},
                    {'Name': 'UserTier', 'Value': user_tier}
                ]
            },
            {
                'MetricName': 'OutputTokens',
                'Value': output_tokens,
                'Unit': 'Count',
                'Timestamp': timestamp,
                'Dimensions': [
                    {'Name': 'Model', 'Value': model.name},
                    {'Name': 'UserTier', 'Value': user_tier}
                ]
            },
            # ë¹„ìš© ì¶”ì 
            {
                'MetricName': 'LLMCost',
                'Value': cost,
                'Unit': 'None',  # USD
                'Timestamp': timestamp,
                'Dimensions': [
                    {'Name': 'Model', 'Value': model.name}
                ]
            },
            # ì§€ì—°ì‹œê°„
            {
                'MetricName': 'LLMLatency',
                'Value': latency_ms,
                'Unit': 'Milliseconds',
                'Timestamp': timestamp,
                'Dimensions': [
                    {'Name': 'Model', 'Value': model.name}
                ]
            },
            # ëª¨ë¸ í˜¸ì¶œ ë¶„í¬
            {
                'MetricName': 'ModelInvocations',
                'Value': 1,
                'Unit': 'Count',
                'Timestamp': timestamp,
                'Dimensions': [
                    {'Name': 'Model', 'Value': model.name}
                ]
            }
        ]

        # ë³µì¡ë„ ì ìˆ˜ (ê³„ì‚°ëœ ê²½ìš°)
        if complexity is not None:
            metrics.append({
                'MetricName': 'QueryComplexity',
                'Value': complexity,
                'Unit': 'None',
                'Timestamp': timestamp,
                'Dimensions': [
                    {'Name': 'Model', 'Value': model.name}
                ]
            })

        # CloudWatchì— ì „ì†¡
        try:
            self.cloudwatch.put_metric_data(
                Namespace=namespace,
                MetricData=metrics
            )
        except Exception as e:
            print(f"âš ï¸ Failed to publish CloudWatch metrics: {e}")


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # LLM Facade ì´ˆê¸°í™”
    llm = LLMFacade(region_name='us-east-1', enable_cloudwatch=True)

    # RAG ì¿¼ë¦¬ ì˜ˆì‹œ
    query = "RAG ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ì„ ìµœì í™”í•˜ëŠ” ë°©ë²•ì€?"
    context_chunks = [
        "RAG ì‹œìŠ¤í…œ ìµœì í™”ë¥¼ ìœ„í•´ì„œëŠ” ë²¡í„° ì¸ë±ì‹±, ìºì‹±, ì²­í¬ í¬ê¸° ì¡°ì •ì´ ì¤‘ìš”í•©ë‹ˆë‹¤...",
        "pgvectorì˜ HNSW ì¸ë±ìŠ¤ë¥¼ ì‚¬ìš©í•˜ë©´ ë¹ ë¥¸ ìœ ì‚¬ë„ ê²€ìƒ‰ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤..."
    ]

    result = llm.invoke(
        query=query,
        context_chunks=context_chunks,
        user_id="user123",
        user_tier="free"
    )

    print(f"âœ… ëª¨ë¸: {result['model_used']}")
    print(f"ğŸ’° ë¹„ìš©: ${result['cost']:.6f}")
    print(f"âš¡ ì§€ì—°ì‹œê°„: {result['latency_ms']}ms")
    print(f"ğŸ“Š ë³µì¡ë„: {result['complexity']:.2f}")
    print(f"ğŸ”„ í´ë°± ë°œìƒ: {result['fallback_occurred']}")
    print(f"\nì‘ë‹µ:\n{result['response']}")
