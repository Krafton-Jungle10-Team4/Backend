"""
ì›Œí¬í”Œë¡œìš° V2 ì‹¤í–‰ ì—”ì§„

í¬íŠ¸ ê¸°ë°˜ ë°ì´í„° íë¦„ê³¼ ë³€ìˆ˜ í’€ì„ ì‚¬ìš©í•˜ëŠ” V2 ì‹¤í–‰ ì—”ì§„ì…ë‹ˆë‹¤.
"""

import asyncio
from typing import Dict, List, Any, Optional, Callable
from collections import deque, defaultdict
from app.core.workflow.base_node_v2 import BaseNodeV2, NodeExecutionContext
from app.core.workflow.variable_pool import VariablePool
from app.core.workflow.service_container import ServiceContainer
from app.core.workflow.node_registry_v2 import node_registry_v2
from app.core.workflow.validator import WorkflowValidator
from app.core.workflow.base_node import NodeStatus
from app.services.vector_service import VectorService
from app.services.llm_service import LLMService
from app.models.workflow_version import WorkflowExecutionRun, WorkflowNodeExecution
from app.models.conversation_variable import ConversationVariable
import logging
from datetime import datetime
import uuid
from sqlalchemy import select

logger = logging.getLogger(__name__)


class WorkflowExecutorV2:
    """
    ì›Œí¬í”Œë¡œìš° V2 ì‹¤í–‰ ì—”ì§„

    í¬íŠ¸ ê¸°ë°˜ ë°ì´í„° íë¦„ì„ ì§€ì›í•˜ëŠ” V2 ë…¸ë“œë“¤ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
    """

    def __init__(self):
        """ì‹¤í–‰ ì—”ì§„ ì´ˆê¸°í™”"""
        self.validator = WorkflowValidator()
        self.nodes: Dict[str, BaseNodeV2] = {}
        self.edges: List[Dict[str, Any]] = []
        self.execution_order: List[str] = []
        self.variable_pool: Optional[VariablePool] = None
        self.service_container: Optional[ServiceContainer] = None
        self.execution_run: Optional[WorkflowExecutionRun] = None
        self.run_start_time: Optional[datetime] = None
        self.workflow_version_id: Optional[str] = None
        # ë…¸ë“œ ì‹¤í–‰ ê¸°ë¡ì„ ë©”ëª¨ë¦¬ì— ì €ì¥ (ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë¬¸ì œ ë°©ì§€)
        self._node_executions_cache: List[WorkflowNodeExecution] = []
        self._virtual_node_aliases = {"conv", "conversation", "env", "environment", "sys", "system"}
        self.cancel_event: Optional[asyncio.Event] = None

    def _is_virtual_node(self, node_id: Optional[str]) -> bool:
        if not node_id:
            return False
        return str(node_id).lower() in self._virtual_node_aliases

    async def execute(
        self,
        workflow_data: Dict[str, Any],
        session_id: str,
        user_message: str,
        bot_id: str,
        user_uuid: str,
        db: Any,
        vector_service: Optional[VectorService] = None,
        llm_service: Optional[LLMService] = None,
        stream_handler: Optional[Any] = None,
        text_normalizer: Optional[Callable[[str], str]] = None,
        initial_node_outputs: Optional[Dict[str, Dict[str, Any]]] = None,
        api_key_id: Optional[str] = None,
        user_id: Optional[str] = None,
        api_request_id: Optional[str] = None,
        cancel_event: Optional[asyncio.Event] = None
    ) -> str:
        """
        V2 ì›Œí¬í”Œë¡œìš° ì‹¤í–‰

        Args:
            workflow_data: ì›Œí¬í”Œë¡œìš° ì •ì˜
            session_id: ì„¸ì…˜ ID
            user_message: ì‚¬ìš©ì ë©”ì‹œì§€
            bot_id: ë´‡ ID
            db: ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜
            vector_service: ë²¡í„° ì„œë¹„ìŠ¤
            llm_service: LLM ì„œë¹„ìŠ¤
            stream_handler: ìŠ¤íŠ¸ë¦¼ í•¸ë“¤ëŸ¬
            text_normalizer: í…ìŠ¤íŠ¸ ì •ê·œí™” í•¨ìˆ˜
            initial_node_outputs: íŠ¹ì • ë…¸ë“œ/í¬íŠ¸ì— ë¯¸ë¦¬ ì£¼ì…í•  ê°’ (nested executionìš©)
            api_key_id: API í‚¤ ID (RESTful API í˜¸ì¶œ ì‹œ)
            user_id: ìµœì¢… ì‚¬ìš©ì ID (RESTful API í˜¸ì¶œ ì‹œ)
            api_request_id: API ìš”ì²­ ID (ì¶”ì ìš©)
            cancel_event: ì‹¤í–‰ ì¤‘ë‹¨ ì‹ í˜¸

        Returns:
            str: ìµœì¢… ì‘ë‹µ

        Raises:
            ValueError: ì›Œí¬í”Œë¡œìš° ê²€ì¦ ì‹¤íŒ¨ ì‹œ
            RuntimeError: ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ì‹œ
        """
        try:
            # ì›Œí¬í”Œë¡œìš° ê²€ì¦
            nodes_data = workflow_data.get("nodes", [])
            edges_data = workflow_data.get("edges", [])

            is_valid, errors, warnings = self.validator.validate(nodes_data, edges_data)
            if not is_valid:
                error_msg = "\n".join(errors)
                raise ValueError(f"V2 ì›Œí¬í”Œë¡œìš° ê²€ì¦ ì‹¤íŒ¨: {error_msg}")

            if warnings:
                for warning in warnings:
                    logger.warning(f"V2 ì›Œí¬í”Œë¡œìš° ê²½ê³ : {warning}")

            self.workflow_version_id = workflow_data.get("workflow_version_id")
            self.cancel_event = cancel_event

            # ë³€ìˆ˜ í’€ ì´ˆê¸°í™”
            environment_vars = workflow_data.get("environment_variables", {})
            conversation_vars = workflow_data.get("conversation_variables", {}) or {}
            persisted_conversation_vars = await self._load_conversation_variables(
                db=db,
                session_id=session_id,
                bot_id=bot_id
            )
            merged_conversation_vars = {**conversation_vars, **persisted_conversation_vars}

            self.variable_pool = VariablePool(
                environment_variables=environment_vars,
                conversation_variables=merged_conversation_vars
            )

            # ì´ˆê¸° ë…¸ë“œ ì¶œë ¥ ì£¼ì… (nested execution ë“±)
            if initial_node_outputs:
                for node_id, ports in initial_node_outputs.items():
                    if not node_id or not isinstance(ports, dict):
                        continue
                    for port_name, value in ports.items():
                        if port_name:
                            self.variable_pool.set_node_output(node_id, port_name, value)

            # ì‹œìŠ¤í…œ ë³€ìˆ˜ ì„¤ì •
            self.variable_pool.set_system_variable("user_message", user_message)
            self.variable_pool.set_system_variable("session_id", session_id)
            self.variable_pool.set_system_variable("bot_id", bot_id)

            # ì„œë¹„ìŠ¤ ì»¨í…Œì´ë„ˆ ì´ˆê¸°í™”
            self.service_container = ServiceContainer()
            self.service_container.register("vector_service", vector_service)
            self.service_container.register("llm_service", llm_service)
            self.service_container.register("bot_id", bot_id)
            self.service_container.register("user_uuid", user_uuid)
            self.service_container.register("session_id", session_id)
            self.service_container.register("db_session", db)
            self.service_container.register("stream_handler", stream_handler)
            self.service_container.register("text_normalizer", text_normalizer)

            # API ë©”íƒ€ë°ì´í„° ë“±ë¡ (ì¤‘ì²© ì›Œí¬í”Œë¡œìš°ì—ì„œ ì ‘ê·¼ ê°€ëŠ¥)
            if api_key_id:
                self.service_container.register("api_key_id", api_key_id)
            if user_id:
                self.service_container.register("user_id", user_id)
            if api_request_id:
                self.service_container.register("api_request_id", api_request_id)

            logger.debug(
                f"ServiceContainer initialized with services: {self.service_container.list_services()}"
            )

            # V2 ë…¸ë“œ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
            self._create_v2_nodes(nodes_data, edges_data)
            self.edges = edges_data

            # ì‹¤í–‰ ìˆœì„œ ê²°ì •
            self.execution_order = self.validator.get_execution_order(nodes_data, edges_data)
            if not self.execution_order:
                raise ValueError("V2 ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ìˆœì„œë¥¼ ê²°ì •í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

            logger.info(f"V2 ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ìˆœì„œ: {self.execution_order}")

            # ì‹¤í–‰ ê¸°ë¡ ì‹œì‘
            self.run_start_time = datetime.utcnow()
            await self._create_execution_run(
                workflow_data=workflow_data,
                session_id=session_id,
                bot_id=bot_id,
                user_message=user_message,
                db=db,
                workflow_version_id=self.workflow_version_id,
                api_key_id=api_key_id,
                user_id=user_id,
                api_request_id=api_request_id
            )

            # ë…¸ë“œ ì‹¤í–‰
            final_response = await self._execute_v2_nodes(stream_handler, text_normalizer)

            # ëŒ€í™” ë³€ìˆ˜ ì €ì¥
            try:
                await self._persist_conversation_variables(
                    db=db,
                    bot_id=bot_id,
                    session_id=session_id
                )
            except Exception as persist_error:
                logger.error(
                    "Failed to persist conversation variables: %s",
                    persist_error
                )

            # ì‹¤í–‰ ê¸°ë¡ ì™„ë£Œ
            await self._finalize_execution_run(
                status="succeeded",
                final_response=final_response,
                db=db
            )

            return final_response

        except asyncio.CancelledError:
            logger.info("V2 ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            if self.execution_run:
                await self._finalize_execution_run(
                    status="failed",
                    error_message="Cancelled by client",
                    db=db
                )
            raise

        except Exception as e:
            logger.error(f"V2 ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")

            # ì‹¤í–‰ ê¸°ë¡ ì‹¤íŒ¨ ì²˜ë¦¬
            if self.execution_run:
                await self._finalize_execution_run(
                    status="failed",
                    error_message=str(e),
                    db=db
                )

            raise RuntimeError(f"V2 ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")

    def _create_v2_nodes(self, nodes_data: List[Dict], edges_data: List[Dict]):
        """
        V2 ë…¸ë“œ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±

        Args:
            nodes_data: ë…¸ë“œ ë°ì´í„°
            edges_data: ì—£ì§€ ë°ì´í„°
        """
        self.nodes = {}

        # ë…¸ë“œ ìƒì„±
        for node_data in nodes_data:
            node_id = node_data.get("id")
            node_type = node_data.get("type")
            config_data = node_data.get("data", {})
            variable_mappings = node_data.get("variable_mappings", {})

            try:
                # V2 ë…¸ë“œ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
                node = node_registry_v2.create_node(
                    node_type=node_type,
                    node_id=node_id,
                    config=config_data,
                    variable_mappings=variable_mappings
                )

                self.nodes[node_id] = node
                logger.debug(f"Created V2 node: {node_id} ({node_type})")

            except Exception as e:
                logger.error(f"V2 ë…¸ë“œ ìƒì„± ì‹¤íŒ¨ ({node_id}): {str(e)}")
                raise ValueError(f"V2 ë…¸ë“œ ìƒì„± ì‹¤íŒ¨ ({node_id}): {str(e)}")

    async def _execute_v2_nodes(
        self,
        stream_handler: Optional[Any] = None,
        text_normalizer: Optional[Callable[[str], str]] = None
    ) -> str:
        """
        V2 ë…¸ë“œë“¤ì„ ìˆœì„œëŒ€ë¡œ ì‹¤í–‰

        Args:
            stream_handler: ìŠ¤íŠ¸ë¦¼ í•¸ë“¤ëŸ¬
            text_normalizer: í…ìŠ¤íŠ¸ ì •ê·œí™” í•¨ìˆ˜

        Returns:
            str: ìµœì¢… ì‘ë‹µ
        """
        final_response = None

        incoming_counts = self._build_incoming_counts()
        edges_by_source = self._group_edges_by_source()
        ready_queue: deque[str] = deque()
        executed_nodes: set[str] = set()

        # ë””ë²„ê·¸: incoming_counts ë¡œê¹…
        logger.info(f"ğŸ“Š Initial incoming_counts: {incoming_counts}")
        logger.info(f"ğŸ“Š Edges by source (keys): {list(edges_by_source.keys())}")
        logger.info(f"ğŸ“Š Total edges: {sum(len(v) for v in edges_by_source.values())}")
        
        # ë””ë²„ê·¸: ê° ë…¸ë“œì˜ ì—£ì§€ ì •ë³´ ìƒì„¸ ì¶œë ¥
        for source, edges in edges_by_source.items():
            edge_details = []
            for edge in edges:
                target = edge.get("target")
                source_port = edge.get("source_port", "default")
                target_port = edge.get("target_port", "")
                edge_details.append(f"{target}[{source_port}â†’{target_port}]")
            logger.info(f"    {source} â†’ {', '.join(edge_details)}")

        for node_id in self.execution_order:
            count = incoming_counts.get(node_id, 0)
            if count == 0:
                ready_queue.append(node_id)
                logger.info(f"âœ… Node {node_id} added to initial ready_queue (incoming_count=0)")
            else:
                logger.info(f"â³ Node {node_id} waiting (incoming_count={count})")
        
        logger.info(f"ğŸ“Š Initial ready_queue: {list(ready_queue)} (size={len(ready_queue)})")

        while ready_queue:
            if self.cancel_event and self.cancel_event.is_set():
                logger.info("ğŸ›‘ Cancellation requested before processing next node.")
                raise asyncio.CancelledError()

            node_id = ready_queue.popleft()
            if node_id in executed_nodes:
                continue

            node = self.nodes.get(node_id)
            if not node:
                logger.warning(f"V2 ë…¸ë“œ {node_id}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                continue

            node_start_time = datetime.utcnow()

            try:
                logger.info(f"V2 ë…¸ë“œ ì‹¤í–‰ ì¤‘: {node_id} ({node.__class__.__name__})")

                if stream_handler:
                    await stream_handler.emit_node_event(
                        node_id=node_id,
                        node_type=node.__class__.__name__,
                        status=NodeStatus.RUNNING.value,
                        message=f"V2 node started"
                    )

                # ì…ë ¥ ì¤€ë¹„
                prepared_inputs = self._gather_node_inputs(node)

                # ì‹¤í–‰ ì»¨í…ìŠ¤íŠ¸ ìƒì„± (ì‹¤í–‰ëœ ë…¸ë“œ ëª©ë¡ ì „ë‹¬)
                context = NodeExecutionContext(
                    node_id=node_id,
                    variable_pool=self.variable_pool,
                    service_container=self.service_container,
                    metadata={"prepared_inputs": prepared_inputs},
                    executed_nodes=list(executed_nodes)  # í˜„ì¬ê¹Œì§€ì˜ ì‹¤í–‰ ê²½ë¡œ ì „ë‹¬
                )

                # ë…¸ë“œ ì‹¤í–‰
                result = await node.execute(context)
                edge_handles = result.metadata.get("edge_handles", []) if result.metadata else []
                process_data = self._extract_process_data(context, node_id)

                # ê²°ê³¼ ì²˜ë¦¬
                if result.status == NodeStatus.COMPLETED and result.output:
                    # ì¶œë ¥ì€ ì´ë¯¸ variable_poolì— ì €ì¥ë¨ (BaseNodeV2.execute()ì—ì„œ)

                    # End ë…¸ë“œì˜ ê²½ìš° ìµœì¢… ì‘ë‹µ ì¶”ì¶œ
                    if node.__class__.__name__ == "EndNodeV2":
                        final_output = result.output.get("final_output", {})
                        final_response = final_output.get("response", final_response)

                logger.info(f"V2 ë…¸ë“œ {node_id} ì‹¤í–‰ ì™„ë£Œ (status={result.status.value})")

                if stream_handler:
                    output_preview = self._summarize_output(result.output, text_normalizer)
                    await stream_handler.emit_node_event(
                        node_id=node_id,
                        node_type=node.__class__.__name__,
                        status=result.status.value,
                        message="V2 node completed" if result.status == NodeStatus.COMPLETED else "V2 node finished",
                        output_preview=output_preview,
                        metadata=process_data
                    )

                # ë…¸ë“œ ì‹¤í–‰ ê¸°ë¡ ì €ì¥
                node_end_time = datetime.utcnow()
                execution_metadata = None
                if context and context.metadata:
                    answer_meta = context.metadata.get("answer")
                    if isinstance(answer_meta, dict):
                        execution_metadata = answer_meta.get(node_id)
                self._create_node_execution(
                    node_id=node_id,
                    node_type=node.__class__.__name__,
                    execution_order=self.execution_order.index(node_id),
                    inputs=prepared_inputs,
                    outputs=result.output,
                    status=result.status.value,
                    error_message=result.error,
                    started_at=node_start_time,
                    finished_at=node_end_time,
                    execution_metadata=execution_metadata,
                    process_data=process_data
                )

                context.metadata.clear()

                if result.status == NodeStatus.FAILED:
                    raise RuntimeError(result.error or f"V2 Node {node_id} failed")

                executed_nodes.add(node_id)
                all_edges_for_node = edges_by_source.get(node_id, [])
                outgoing_edges = self._select_outgoing_edges(
                    all_edges_for_node,
                    edge_handles
                )
                
                # ğŸ”¥ í•µì‹¬ ìˆ˜ì •: ë¶„ê¸° ë…¸ë“œ ì²˜ë¦¬ (IfElse, QuestionClassifier ë“±)
                # ì„ íƒë˜ì§€ ì•Šì€ ë¶„ê¸°ì˜ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ë…¸ë“œ ì˜ì¡´ì„± í•´ì†Œ
                if edge_handles and all_edges_for_node:
                    unselected_edges = self._get_unselected_edges(
                        all_edges_for_node,
                        edge_handles
                    )
                    if unselected_edges:
                        logger.info(
                            "ğŸ”€ Branch node %s: resolving unselected branches (%d edges)",
                            node_id,
                            len(unselected_edges)
                        )
                        self._resolve_unselected_branch_dependencies(
                            unselected_edges,
                            incoming_counts,
                            ready_queue,
                            executed_nodes
                        )
                
                # ìƒì„¸í•œ ì—£ì§€ ë§¤ì¹­ ë¡œê·¸
                logger.info(
                    "ğŸ”— Node %s executed:\n"
                    "  - edge_handles: %s\n"
                    "  - total_edges: %d\n"
                    "  - selected_edges: %d\n"
                    "  - current ready_queue size: %d",
                    node_id,
                    edge_handles,
                    len(all_edges_for_node),
                    len(outgoing_edges),
                    len(ready_queue)
                )
                
                # ì—£ì§€ ë§¤ì¹­ ìƒì„¸ ì •ë³´
                if all_edges_for_node and not outgoing_edges and edge_handles:
                    logger.warning(
                        "âš ï¸  Node %s: edge_handles=%s but no edges matched! Available source_ports: %s",
                        node_id,
                        edge_handles,
                        [e.get("source_port") for e in all_edges_for_node]
                    )
                for edge in outgoing_edges:
                    target = edge.get("target")
                    source_port = edge.get("source_port", "default")
                    target_port = edge.get("target_port", "")
                    logger.info(f"  â†’ Processing edge: {node_id}[{source_port}] -> {target}[{target_port}]")
                    if self._is_virtual_node(target) or target not in self.nodes:
                        logger.debug(f"  â­ï¸  Skipping edge to {target} (virtual or not in nodes)")
                        continue
                    previous_count = incoming_counts.get(target, 0)
                    incoming_counts[target] = max(previous_count - 1, 0)
                    logger.info(
                        "  âœ… Edge %s -> %s resolved: incoming_count %d -> %d",
                        node_id,
                        target,
                        previous_count,
                        incoming_counts[target]
                    )
                    if incoming_counts[target] == 0:
                        ready_queue.append(target)
                        logger.info("  ğŸ¯ Node %s added to ready_queue (all dependencies resolved)", target)
                    else:
                        logger.info(f"  â³ Node {target} still waiting ({incoming_counts[target]} dependencies remaining)")
                
                # ë£¨í”„ ëì—ì„œ í˜„ì¬ ìƒíƒœ ìš”ì•½
                waiting_nodes = {k: v for k, v in incoming_counts.items() if v > 0 and k not in executed_nodes}
                logger.info(
                    "ğŸ“Š After node %s: ready_queue=%s, waiting_nodes=%s, executed=%d/%d",
                    node_id,
                    list(ready_queue),
                    waiting_nodes,
                    len(executed_nodes),
                    len(self.nodes)
                )

            except asyncio.CancelledError:
                logger.info(f"ğŸ›‘ Node execution cancelled: {node_id}")
                raise

            except Exception as e:
                logger.error(f"V2 ë…¸ë“œ {node_id} ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")
                node.set_status(NodeStatus.FAILED)

                # ì‹¤íŒ¨í•œ ë…¸ë“œ ê¸°ë¡ ì €ì¥
                node_end_time = datetime.utcnow()
                execution_metadata = None
                process_data = None
                if 'context' in locals() and context and context.metadata:
                    answer_meta = context.metadata.get("answer")
                    if isinstance(answer_meta, dict):
                        execution_metadata = answer_meta.get(node_id)
                    process_data = self._extract_process_data(context, node_id)
                self._create_node_execution(
                    node_id=node_id,
                    node_type=node.__class__.__name__,
                    execution_order=self.execution_order.index(node_id),
                    inputs=prepared_inputs if 'prepared_inputs' in locals() else {},
                    outputs={},
                    status=NodeStatus.FAILED.value,
                    error_message=str(e),
                    started_at=node_start_time,
                    finished_at=node_end_time,
                    execution_metadata=execution_metadata,
                    process_data=process_data
                )

                if stream_handler:
                    await stream_handler.emit_node_event(
                        node_id=node_id,
                        node_type=node.__class__.__name__,
                        status=NodeStatus.FAILED.value,
                        message=str(e),
                        metadata=process_data
                    )
                if 'context' in locals() and context:
                    context.metadata.clear()
                raise

        # ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì™„ë£Œ í›„ ë¯¸ì‹¤í–‰ ë…¸ë“œ í™•ì¸
        unexecuted_nodes = set(self.nodes.keys()) - executed_nodes
        if unexecuted_nodes:
            logger.warning(
                "âš ï¸  Workflow completed but some nodes were not executed:\n"
                "  - executed: %d/%d nodes\n"
                "  - unexecuted nodes: %s\n"
                "  - final incoming_counts: %s",
                len(executed_nodes),
                len(self.nodes),
                list(unexecuted_nodes),
                {k: v for k, v in incoming_counts.items() if k in unexecuted_nodes}
            )
        else:
            logger.info(f"âœ… All {len(executed_nodes)} nodes executed successfully")

        # ìµœì¢… ì‘ë‹µì´ ì—†ìœ¼ë©´ ì‹¤í–‰ëœ LLM ë…¸ë“œì˜ responseë¥¼ ì°¾ì•„ì„œ ì‚¬ìš©
        if not final_response:
            logger.info("End ë…¸ë“œì—ì„œ ìµœì¢… ì‘ë‹µì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. LLM ë…¸ë“œì˜ responseë¥¼ ì°¾ëŠ” ì¤‘...")
            for node_id in executed_nodes:
                node = self.nodes.get(node_id)
                if node and node.__class__.__name__ == "LLMNodeV2":
                    llm_outputs = self.variable_pool.get_all_node_outputs(node_id)
                    if llm_outputs and "response" in llm_outputs:
                        final_response = llm_outputs["response"]
                        logger.info(f"âœ… LLM ë…¸ë“œ {node_id}ì˜ responseë¥¼ ìµœì¢… ì‘ë‹µìœ¼ë¡œ ì‚¬ìš©: {len(final_response)} chars")
                        break
        
        # ì—¬ì „íˆ ì—†ìœ¼ë©´ ì—ëŸ¬
        if not final_response:
            logger.error(
                "âŒ ìµœì¢… ì‘ë‹µì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤! "
                "End ë…¸ë“œê°€ ì‹¤í–‰ë˜ì§€ ì•Šì•˜ê±°ë‚˜, LLM ë…¸ë“œì˜ responseê°€ ì—†ìŠµë‹ˆë‹¤. "
                "ì‹¤í–‰ëœ ë…¸ë“œ: %s",
                list(executed_nodes)
            )
            final_response = ""  # ë¹ˆ ë¬¸ìì—´ ë°˜í™˜ (ê¸°ë³¸ ë©”ì‹œì§€ ì œê±°)

        return final_response

    async def _load_conversation_variables(
        self,
        db: Any,
        session_id: str,
        bot_id: str,
    ) -> Dict[str, Any]:
        """DBì—ì„œ ëŒ€í™” ë³€ìˆ˜ ë¡œë“œ"""
        if not db or not session_id:
            return {}

        stmt = (
            select(ConversationVariable)
            .where(ConversationVariable.conversation_id == session_id)
            .where(ConversationVariable.bot_id == bot_id)
        )
        result = await db.execute(stmt)
        records = result.scalars().all()
        return {record.key: record.value for record in records}

    async def _persist_conversation_variables(
        self,
        db: Any,
        bot_id: str,
        session_id: str,
    ) -> None:
        """ë³€ê²½ëœ ëŒ€í™” ë³€ìˆ˜ë¥¼ DBì— ì €ì¥"""
        if not db or not self.variable_pool or not session_id:
            return

        dirty_vars = self.variable_pool.get_dirty_conversation_variables()
        if not dirty_vars:
            return

        for key, value in dirty_vars.items():
            stmt = (
                select(ConversationVariable)
                .where(ConversationVariable.conversation_id == session_id)
                .where(ConversationVariable.bot_id == bot_id)
                .where(ConversationVariable.key == key)
            )
            result = await db.execute(stmt)
            record = result.scalar_one_or_none()

            if record:
                record.value = value
                record.updated_at = datetime.utcnow()
            else:
                db.add(
                    ConversationVariable(
                        conversation_id=session_id,
                        bot_id=bot_id,
                        key=key,
                        value=value,
                    )
                )

        await db.flush()
        self.variable_pool.clear_conversation_variable_dirty()

    def _build_incoming_counts(self) -> Dict[str, int]:
        """ì˜ì¡´ì„± ì¹´ìš´íŠ¸ ê³„ì‚° (ê°€ìƒ ë…¸ë“œ ì—£ì§€ ì œì™¸)"""
        counts: Dict[str, int] = {node_id: 0 for node_id in self.nodes.keys()}
        
        # ë””ë²„ê·¸: Start ë…¸ë“œ ì°¾ê¸°
        start_nodes = [nid for nid, node in self.nodes.items() 
                       if node.__class__.__name__ == 'StartNodeV2']
        
        logger.info(f"ğŸ” Building incoming_counts from {len(self.edges)} edges...")
        virtual_edges = 0
        invalid_edges = 0
        valid_edges = 0
        start_bypass_count = 0  # Start ë…¸ë“œê°€ ì¡°ê±´ ë¶„ê¸°ë¥¼ ìš°íšŒí•œ ì˜ëª»ëœ ì—°ê²°
        
        for edge in self.edges:
            source = edge.get("source")
            target = edge.get("target")
            source_port = edge.get("source_port", "")
            target_port = edge.get("target_port", "")
            
            if self._is_virtual_node(source) or self._is_virtual_node(target):
                virtual_edges += 1
                logger.debug(f"  â­ï¸  Skipping virtual edge: {source}[{source_port}] -> {target}[{target_port}]")
                continue
            
            # Start ë…¸ë“œì˜ ì˜ëª»ëœ ì—°ê²° ê°ì§€
            if source in start_nodes and target in self.nodes:
                target_node = self.nodes[target]
                target_type = target_node.__class__.__name__
                
                # Startê°€ ì¡°ê±´ ë¶„ê¸° ë…¸ë“œê°€ ì•„ë‹Œ ë…¸ë“œì— ì§ì ‘ ì—°ê²°ëœ ê²½ìš°
                if target_type not in ['IfElseNodeV2', 'QuestionClassifierNodeV2']:
                    # í•´ë‹¹ ë…¸ë“œê°€ ë‹¤ë¥¸ ë…¸ë“œë¡œë¶€í„°ë„ incoming ì—£ì§€ë¥¼ ë°›ëŠ”ì§€ í™•ì¸
                    other_incoming = sum(
                        1 for e in self.edges 
                        if e.get("target") == target and e.get("source") != source and e.get("source") in self.nodes
                    )
                    
                    if other_incoming > 0:
                        start_bypass_count += 1
                        logger.warning(
                            f"  âš ï¸  SUSPICIOUS START EDGE: {source}[{source_port}] â†’ {target}[{target_port}] "
                            f"(target type: {target_type}, other_incoming: {other_incoming}). "
                            f"This may bypass branching logic!"
                        )
            
            if source in self.nodes and target in self.nodes:
                counts[target] = counts.get(target, 0) + 1
                counts.setdefault(source, counts.get(source, 0))
                valid_edges += 1
                logger.debug(f"  âœ… Edge {source} -> {target}: incoming_count[{target}] = {counts[target]}")
            else:
                invalid_edges += 1
                missing = []
                if source not in self.nodes:
                    missing.append(f"source '{source}'")
                if target not in self.nodes:
                    missing.append(f"target '{target}'")
                logger.warning(f"  âš ï¸  Invalid edge {source} -> {target}: {', '.join(missing)} not in nodes")
        
        logger.info(
            f"ğŸ” Edge processing summary: valid={valid_edges}, virtual={virtual_edges}, invalid={invalid_edges}"
        )
        
        if start_bypass_count > 0:
            logger.warning(
                f"âš ï¸  Found {start_bypass_count} suspicious Start node connections that may "
                f"bypass branching logic. This is likely a frontend workflow editor issue."
            )
        
        return counts

    def _group_edges_by_source(self) -> Dict[str, List[Dict[str, Any]]]:
        mapping: Dict[str, List[Dict[str, Any]]] = defaultdict(list)
        for edge in self.edges:
            source = edge.get("source")
            target = edge.get("target")
            if self._is_virtual_node(source) or self._is_virtual_node(target):
                continue
            if source in self.nodes and target in self.nodes:
                mapping[source].append(edge)
        return mapping

    def _select_outgoing_edges(
        self,
        edges: List[Dict[str, Any]],
        handles: Optional[List[str]]
    ) -> List[Dict[str, Any]]:
        if not edges:
            return []
        if not handles:
            return edges

        normalized = [handle for handle in handles if handle]
        if not normalized:
            return edges

        selected = [edge for edge in edges if edge.get("source_port") in normalized]
        if selected:
            return selected
        return edges

    def _get_unselected_edges(
        self,
        edges: List[Dict[str, Any]],
        selected_handles: List[str]
    ) -> List[Dict[str, Any]]:
        """
        ì„ íƒë˜ì§€ ì•Šì€ ë¶„ê¸°ì˜ ì—£ì§€ë“¤ì„ ë°˜í™˜
        
        Args:
            edges: ë…¸ë“œì˜ ëª¨ë“  outgoing ì—£ì§€
            selected_handles: ì„ íƒëœ ë¶„ê¸°ì˜ í•¸ë“¤ (ì˜ˆ: ['if'])
            
        Returns:
            ì„ íƒë˜ì§€ ì•Šì€ ë¶„ê¸°ì˜ ì—£ì§€ ë¦¬ìŠ¤íŠ¸
        """
        if not edges or not selected_handles:
            return []
        
        # selected_handlesì— í¬í•¨ë˜ì§€ ì•Šì€ ì—£ì§€ë“¤ë§Œ ë°˜í™˜
        unselected = [
            edge for edge in edges 
            if edge.get("source_port") and edge.get("source_port") not in selected_handles
        ]
        
        return unselected

    def _resolve_unselected_branch_dependencies(
        self,
        unselected_edges: List[Dict[str, Any]],
        incoming_counts: Dict[str, int],
        ready_queue: deque,
        executed_nodes: set
    ) -> None:
        """
        ì„ íƒë˜ì§€ ì•Šì€ ë¶„ê¸°ì˜ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ë…¸ë“œë“¤ì˜ ì˜ì¡´ì„±ì„ ì¬ê·€ì ìœ¼ë¡œ í•´ì†Œ
        
        âš ï¸ ì¤‘ìš”: ì´ ë©”ì„œë“œëŠ” ë…¸ë“œë¥¼ ready_queueì— ì¶”ê°€í•˜ì§€ ì•Šê³ ,
        incoming_countë§Œ ê°ì†Œì‹œí‚µë‹ˆë‹¤. ë…¸ë“œë¥¼ ì‹¤í–‰í•˜ë ¤ë©´ ë‹¤ë¥¸ ê²½ë¡œë¥¼ í†µí•´
        incoming_countê°€ 0ì´ ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
        
        Args:
            unselected_edges: ì„ íƒë˜ì§€ ì•Šì€ ë¶„ê¸°ì˜ ì—£ì§€ë“¤
            incoming_counts: ë…¸ë“œë³„ ë‚¨ì€ ì˜ì¡´ì„± ì¹´ìš´íŠ¸
            ready_queue: ì‹¤í–‰ ëŒ€ê¸° í
            executed_nodes: ì´ë¯¸ ì‹¤í–‰ëœ ë…¸ë“œ ì§‘í•©
        """
        # ì²˜ë¦¬í•  ë…¸ë“œë“¤ (BFS ë°©ì‹)
        nodes_to_process: deque = deque()
        processed: set = set()
        
        # ì„ íƒë˜ì§€ ì•Šì€ ë¶„ê¸°ì˜ ì§ì ‘ íƒ€ê²Ÿ ë…¸ë“œë“¤ì„ íì— ì¶”ê°€
        for edge in unselected_edges:
            target = edge.get("target")
            if target and not self._is_virtual_node(target) and target in self.nodes:
                nodes_to_process.append(target)
                logger.debug(f"    â­ï¸  Marking unselected branch target: {target}")
        
        # BFSë¡œ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ë…¸ë“œë“¤ì˜ ì˜ì¡´ì„± í•´ì†Œ
        edges_by_source = self._group_edges_by_source()
        
        while nodes_to_process:
            current_node = nodes_to_process.popleft()
            
            # ì´ë¯¸ ì²˜ë¦¬í–ˆê±°ë‚˜ ì‹¤í–‰ëœ ë…¸ë“œëŠ” ìŠ¤í‚µ
            if current_node in processed or current_node in executed_nodes:
                continue
            
            processed.add(current_node)
            
            # ì´ ë…¸ë“œì˜ incoming_countë¥¼ 0ìœ¼ë¡œ ì„¤ì •
            # (ì„ íƒë˜ì§€ ì•Šì€ ë¶„ê¸°ì˜ ë…¸ë“œì´ë¯€ë¡œ ì‹¤í–‰ë˜ì§€ ì•ŠìŒ)
            old_count = incoming_counts.get(current_node, 0)
            if old_count > 0:
                incoming_counts[current_node] = 0
                logger.info(
                    f"    ğŸ”€ Unselected branch node {current_node}: "
                    f"incoming_count {old_count} -> 0 (branch not taken, will NOT be added to ready_queue)"
                )
            
            # âš ï¸ ì¤‘ìš”: ready_queueì— ì¶”ê°€í•˜ì§€ ì•ŠìŒ!
            # ì„ íƒë˜ì§€ ì•Šì€ ë¶„ê¸°ì˜ ë…¸ë“œëŠ” ì‹¤í–‰ë˜ì–´ì„œëŠ” ì•ˆ ë¨
            
            # ì´ ë…¸ë“œì˜ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ë…¸ë“œë“¤ ì²˜ë¦¬
            outgoing = edges_by_source.get(current_node, [])
            for edge in outgoing:
                downstream = edge.get("target")
                if not downstream or self._is_virtual_node(downstream) or downstream not in self.nodes:
                    continue
                
                # ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ë…¸ë“œì˜ incoming_count ê°ì†Œ
                prev_count = incoming_counts.get(downstream, 0)
                if prev_count > 0:
                    incoming_counts[downstream] = prev_count - 1
                    logger.debug(
                        f"      â¬‡ï¸  Downstream {downstream}: "
                        f"incoming_count {prev_count} -> {incoming_counts[downstream]}"
                    )
                    
                    # âš ï¸ ë³€ê²½: ready_queueì— ì¶”ê°€í•˜ì§€ ì•ŠìŒ!
                    # ë‹¤ë¥¸ ê²½ë¡œë¥¼ í†µí•´ incoming_countê°€ 0ì´ ë˜ë©´ ìì—°ìŠ¤ëŸ½ê²Œ ì‹¤í–‰ë¨
                
                # ì´ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ë…¸ë“œë„ ì²˜ë¦¬ ëŒ€ìƒì— ì¶”ê°€
                # (ì„ íƒë˜ì§€ ì•Šì€ ë¶„ê¸°ì˜ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ì¼ ìˆ˜ ìˆìŒ)
                if downstream not in processed:
                    nodes_to_process.append(downstream)

    @staticmethod
    def _extract_process_data(context: Optional[NodeExecutionContext], node_id: str) -> Optional[Dict[str, Any]]:
        if not context or not context.metadata:
            return None
        data: Dict[str, Any] = {}
        for key, value in context.metadata.items():
            if isinstance(value, dict) and node_id in value:
                data[key] = value[node_id]
            elif key == node_id:
                data[key] = value
        return data or None

    def _gather_node_inputs(self, node: BaseNodeV2) -> Dict[str, Any]:
        """
        ë…¸ë“œì˜ ì…ë ¥ í¬íŠ¸ ê°’ë“¤ì„ ìˆ˜ì§‘

        Args:
            node: V2 ë…¸ë“œ

        Returns:
            Dict[str, Any]: {port_name: value} í˜•ì‹ì˜ ì…ë ¥
        """
        prepared_inputs = {}

        # variable_mappingsì—ì„œ ê° ì…ë ¥ í¬íŠ¸ì˜ ì†ŒìŠ¤ë¥¼ ì°¾ì•„ í•´ì„
        for port_name, mapping in node.variable_mappings.items():
            # mappingì€ ValueSelector í˜•ì‹ ë˜ëŠ” ì§ì ‘ ë³€ìˆ˜ ì´ë¦„
            if isinstance(mapping, dict):
                # ValueSelector ê°ì²´ í˜•ì‹
                selector = mapping.get("variable") or mapping.get("source", {}).get("variable")
            else:
                # ë¬¸ìì—´ í˜•ì‹
                selector = mapping

            if selector:
                try:
                    value = self.variable_pool.resolve_value_selector(selector)
                    prepared_inputs[port_name] = value
                    # ë””ë²„ê·¸ ë¡œê¹…: LLM ë…¸ë“œì˜ context ì…ë ¥ì— ëŒ€í•´ ìƒì„¸ ë¡œê¹…
                    if node.__class__.__name__ == "LLMNodeV2" and port_name == "context":
                        if value:
                            logger.info(f"[LLMNodeV2] Context input resolved: {len(str(value))} chars from '{selector}'")
                        else:
                            logger.warning(f"[LLMNodeV2] Context input is empty or None from '{selector}'")
                            # conversation.resultê°€ ìˆëŠ”ì§€ í™•ì¸
                            try:
                                result_value = self.variable_pool.resolve_value_selector("conversation.result")
                                if result_value:
                                    logger.info(f"[LLMNodeV2] Found conversation.result: {len(str(result_value))} chars (but context input is empty)")
                                else:
                                    logger.warning(f"[LLMNodeV2] conversation.result is also empty or not found")
                            except Exception as e:
                                logger.debug(f"[LLMNodeV2] Could not check conversation.result: {e}")
                except Exception as e:
                    logger.warning(f"Failed to resolve input '{port_name}' from '{selector}': {e}")
                    prepared_inputs[port_name] = None
            else:
                logger.debug(f"No selector found for input port '{port_name}' in node {node.node_id}")

        # LLM ë…¸ë“œì˜ ê²½ìš° variable_mappingsì— ì—†ëŠ” ì…ë ¥ í¬íŠ¸ë„ í™•ì¸
        if node.__class__.__name__ == "LLMNodeV2":
            schema = node.get_port_schema()
            for port_def in schema.inputs:
                port_name = port_def.name
                if port_name not in prepared_inputs:
                    logger.debug(f"[LLMNodeV2] Input port '{port_name}' not in variable_mappings")

        return prepared_inputs

    @staticmethod
    def _summarize_output(
        output: Any,
        normalizer: Optional[Callable[[str], str]] = None
    ) -> Optional[str]:
        """ë…¸ë“œ ì¶œë ¥ ìš”ì•½ ë¬¸ìì—´ ìƒì„±"""
        summary: Optional[str] = None

        if isinstance(output, dict):
            # V2 ë…¸ë“œì˜ ì¶œë ¥ í¬íŠ¸ ì²˜ë¦¬
            if "response" in output:
                summary = output["response"]
            elif "context" in output:
                summary = f"Context prepared ({len(output.get('context', ''))} chars)"
            elif "query" in output:
                summary = output["query"]
            elif "final_output" in output:
                final = output["final_output"]
                if isinstance(final, dict):
                    summary = final.get("response", str(final))
        elif isinstance(output, str):
            summary = output

        if summary and normalizer:
            summary = normalizer(summary)

        if summary:
            return summary[:200]
        return None

    def get_node_status(self, node_id: str) -> Optional[NodeStatus]:
        """
        ë…¸ë“œ ìƒíƒœ ì¡°íšŒ

        Args:
            node_id: ë…¸ë“œ ID

        Returns:
            NodeStatus ë˜ëŠ” None
        """
        node = self.nodes.get(node_id)
        return node.status if node else None

    def get_all_node_statuses(self) -> Dict[str, NodeStatus]:
        """
        ëª¨ë“  ë…¸ë“œì˜ ìƒíƒœ ì¡°íšŒ

        Returns:
            Dict[str, NodeStatus]: ë…¸ë“œ IDì™€ ìƒíƒœ ë§µ
        """
        return {node_id: node.status for node_id, node in self.nodes.items()}

    async def _create_execution_run(
        self,
        workflow_data: Dict[str, Any],
        session_id: str,
        bot_id: str,
        user_message: str,
        db: Any,
        workflow_version_id: Optional[str] = None,
        api_key_id: Optional[str] = None,
        user_id: Optional[str] = None,
        api_request_id: Optional[str] = None
    ) -> None:
        """
        ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ê¸°ë¡ ìƒì„±

        Args:
            workflow_data: ì›Œí¬í”Œë¡œìš° ì •ì˜
            session_id: ì„¸ì…˜ ID
            bot_id: ë´‡ ID
            user_message: ì‚¬ìš©ì ë©”ì‹œì§€
            db: ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜
            workflow_version_id: ì›Œí¬í”Œë¡œìš° ë²„ì „ ID
            api_key_id: API í‚¤ ID (RESTful API í˜¸ì¶œ ì‹œ)
            user_id: ìµœì¢… ì‚¬ìš©ì ID (RESTful API í˜¸ì¶œ ì‹œ)
            api_request_id: API ìš”ì²­ ID (ì¶”ì ìš©)
        """
        if not db:
            logger.warning("DB ì„¸ì…˜ì´ ì—†ì–´ ì‹¤í–‰ ê¸°ë¡ì„ ì €ì¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")
            return

        try:
            version_uuid = None
            if workflow_version_id:
                try:
                    version_uuid = uuid.UUID(str(workflow_version_id))
                except (ValueError, TypeError):
                    logger.warning("Invalid workflow_version_id provided: %s", workflow_version_id)

            self.execution_run = WorkflowExecutionRun(
                id=uuid.uuid4(),
                bot_id=bot_id,
                workflow_version_id=version_uuid,
                session_id=session_id,
                graph_snapshot=workflow_data,
                inputs={"user_message": user_message},
                outputs={},
                status="running",
                started_at=self.run_start_time,
                total_steps=len(self.execution_order),
                # API ì „ìš© í•„ë“œ ì„¤ì •
                api_key_id=api_key_id,
                user_id=user_id,
                api_request_id=api_request_id
            )

            db.add(self.execution_run)
            await db.commit()
            logger.info(
                f"V2 ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ê¸°ë¡ ìƒì„±: run_id={self.execution_run.id}, "
                f"api_key_id={api_key_id}, user_id={user_id}"
            )

        except Exception as e:
            logger.error(f"ì‹¤í–‰ ê¸°ë¡ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            await db.rollback()

    async def _finalize_execution_run(
        self,
        status: str,
        final_response: Optional[str] = None,
        error_message: Optional[str] = None,
        db: Any = None
    ) -> None:
        """
        ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ê¸°ë¡ ì™„ë£Œ

        Args:
            status: ì‹¤í–‰ ìƒíƒœ (completed, failed)
            final_response: ìµœì¢… ì‘ë‹µ
            error_message: ì—ëŸ¬ ë©”ì‹œì§€
            db: ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜
        """
        if not self.execution_run or not db:
            return

        try:
            finished_at = datetime.utcnow()
            elapsed_ms = int((finished_at - self.run_start_time).total_seconds() * 1000)

            self.execution_run.status = status
            self.execution_run.finished_at = finished_at
            self.execution_run.elapsed_time = elapsed_ms

            if final_response:
                self.execution_run.outputs = {"final_response": final_response}

            if error_message:
                self.execution_run.error_message = error_message

            # ìºì‹œëœ ë…¸ë“œ ì‹¤í–‰ ê¸°ë¡ë“¤ì„ DBì— ì €ì¥
            # node_executions ê´€ê³„ì— ì ‘ê·¼í•˜ì§€ ì•Šê³  db.add()ë§Œ ì‚¬ìš© (ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë¬¸ì œ ë°©ì§€)
            if self._node_executions_cache:
                for node_exec in self._node_executions_cache:
                    # workflow_run_idëŠ” ì´ë¯¸ ì„¤ì •ë˜ì–´ ìˆìœ¼ë¯€ë¡œ ê´€ê³„ ì ‘ê·¼ ë¶ˆí•„ìš”
                    db.add(node_exec)

            # í† í° í•©ê³„ ê³„ì‚° (ë…¸ë“œ ì‹¤í–‰ ê¸°ë¡ì—ì„œ)
            total_tokens = sum(
                node_exec.tokens_used or 0
                for node_exec in self._node_executions_cache
            )
            self.execution_run.total_tokens = total_tokens

            await db.commit()
            logger.info(
                f"V2 ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì™„ë£Œ: run_id={self.execution_run.id}, "
                f"status={status}, elapsed={elapsed_ms}ms"
            )

        except Exception as e:
            logger.error(f"ì‹¤í–‰ ê¸°ë¡ ì™„ë£Œ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
            await db.rollback()

    def _create_node_execution(
        self,
        node_id: str,
        node_type: str,
        execution_order: int,
        inputs: Dict[str, Any],
        outputs: Dict[str, Any],
        status: str,
        error_message: Optional[str],
        started_at: datetime,
        finished_at: datetime,
        execution_metadata: Optional[Dict[str, Any]] = None,
        process_data: Optional[Dict[str, Any]] = None,
    ) -> None:
        """
        ë…¸ë“œ ì‹¤í–‰ ê¸°ë¡ ìƒì„±

        Args:
            node_id: ë…¸ë“œ ID
            node_type: ë…¸ë“œ íƒ€ì…
            execution_order: ì‹¤í–‰ ìˆœì„œ
            inputs: ì…ë ¥ ë°ì´í„°
            outputs: ì¶œë ¥ ë°ì´í„°
            status: ì‹¤í–‰ ìƒíƒœ
            error_message: ì—ëŸ¬ ë©”ì‹œì§€
            started_at: ì‹œì‘ ì‹œê°„
            finished_at: ì¢…ë£Œ ì‹œê°„
            execution_metadata: ì‹¤í–‰ ë©”íƒ€ë°ì´í„° (Answer ë…¸ë“œ ë Œë”ë§ ì •ë³´ ë“±)
        """
        if not self.execution_run:
            return

        try:
            elapsed_ms = int((finished_at - started_at).total_seconds() * 1000)

            # í† í° ì‚¬ìš©ëŸ‰ ì¶”ì¶œ (LLM ë…¸ë“œì˜ ê²½ìš°)
            tokens_used = 0
            if node_type == "LLMNodeV2" and outputs:
                tokens_used = outputs.get("tokens", 0)

            # ì¶œë ¥ ë°ì´í„°ì— ì‹¤í–‰ ë©”íƒ€ë°ì´í„° ë³‘í•©
            final_outputs = outputs.copy() if outputs else {}
            if execution_metadata:
                final_outputs["_execution_metadata"] = execution_metadata

            node_execution = WorkflowNodeExecution(
                id=uuid.uuid4(),
                workflow_run_id=self.execution_run.id,
                node_id=node_id,
                node_type=node_type,
                execution_order=execution_order,
                inputs=inputs,
                outputs=final_outputs,
                process_data=process_data,
                status=status,
                error_message=error_message,
                started_at=started_at,
                finished_at=finished_at,
                elapsed_time=elapsed_ms,
                tokens_used=tokens_used
            )

            # ë©”ëª¨ë¦¬ ìºì‹œì— ì¶”ê°€ (ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë¬¸ì œ ë°©ì§€)
            # ë‚˜ì¤‘ì— _finalize_execution_runì—ì„œ í•œ ë²ˆì— DBì— ì €ì¥
            self._node_executions_cache.append(node_execution)

            logger.debug(
                f"ë…¸ë“œ ì‹¤í–‰ ê¸°ë¡ ìƒì„±: node_id={node_id}, "
                f"status={status}, elapsed={elapsed_ms}ms"
            )

        except Exception as e:
            logger.error(f"ë…¸ë“œ ì‹¤í–‰ ê¸°ë¡ ìƒì„± ì‹¤íŒ¨: {str(e)}")
