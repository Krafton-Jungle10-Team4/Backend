"""
AWS Bedrock (Anthropic Claude) API í´ë¼ì´ì–¸íŠ¸ êµ¬í˜„
"""
from typing import List, Dict, AsyncGenerator, Optional
import logging
import json
import asyncio
from concurrent.futures import ThreadPoolExecutor
import boto3
from botocore.exceptions import ClientError, BotoCoreError

from app.core.llm_base import BaseLLMClient
from app.core.llm_registry import register_provider
from app.core.providers.config import BedrockConfig
from app.core.exceptions import (
    LLMAPIError,
    LLMRateLimitError,
)
from app.core.llm_rate_limiter import LLMRateLimiter

logger = logging.getLogger(__name__)


@register_provider("bedrock")
class BedrockClient(BaseLLMClient):
    """
    AWS Bedrock (Claude) API í´ë¼ì´ì–¸íŠ¸
    
    ë™ì‹œì„± ì²˜ë¦¬:
    - ë¹„ë™ê¸°/ë…¼ë¸”ë¡œí‚¹: FastAPIì˜ async/await ì‚¬ìš©
    - ThreadPoolExecutor: boto3 ë™ê¸° í˜¸ì¶œì„ ìŠ¤ë ˆë“œ í’€ì—ì„œ ì‹¤í–‰
    - Semaphore: ë™ì‹œ ìš”ì²­ ìˆ˜ ì œí•œ (í”„ë¡œë¹„ì €ë‹ëœ ìš©ëŸ‰ ë³´í˜¸)
    """

    # í´ë˜ìŠ¤ ë ˆë²¨ ThreadPoolExecutor (ëª¨ë“  ì¸ìŠ¤í„´ìŠ¤ ê³µìœ )
    # í”„ë¡œë¹„ì €ë‹ëœ ìš©ëŸ‰ 1 MU ê¸°ì¤€: ë™ì‹œ ìš”ì²­ 10-20ê°œ ì •ë„ ì²˜ë¦¬ ê°€ëŠ¥
    _executor: Optional[ThreadPoolExecutor] = None
    _executor_lock = asyncio.Lock()
    
    # ë™ì‹œì„± ì œí•œ: Rate Limit ë³´í˜¸ ë° ë¹„ìš© ê´€ë¦¬
    # ON_DEMAND ëª¨ë“œ: 10ê°œ ë™ì‹œ ìš”ì²­ (Rate Limit ë³´í˜¸)
    # í”„ë¡œë¹„ì €ë‹ ëª¨ë“œ: 1 MU = ì•½ 15ê°œ ë™ì‹œ ìš”ì²­ ì²˜ë¦¬ ê°€ëŠ¥
    _semaphore: Optional[asyncio.Semaphore] = None
    _max_concurrent_requests: Optional[int] = None  # ë™ì ìœ¼ë¡œ ê³„ì‚°ë¨
    _provisioned_model_units: int = 0  # í”„ë¡œë¹„ì €ë‹ëœ ìš©ëŸ‰ (Model Units)

    def __init__(self, config: BedrockConfig):
        self.config = config
        self.client = boto3.client(
            'bedrock-runtime',
            region_name=config.region_name
        )
        self.model = config.default_model
        self.system_prompt = (
            config.system_prompt
            or "ë‹¹ì‹ ì€ ìœ ëŠ¥í•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì‚¬ìš©ìì—ê²Œ ì¹œì ˆí•˜ê³  ëª…í™•í•˜ê²Œ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤."
        )
        
        # í”„ë¡œë¹„ì €ë‹ëœ ìš©ëŸ‰ í™•ì¸ ë° ë™ì‹œì„± ì œí•œ ê³„ì‚°
        from app.config import settings
        provisioned_units = getattr(settings, 'bedrock_provisioned_model_units', 0) or 0
        
        # ë™ì‹œì„± ì œí•œ ê³„ì‚°: 1 MU = 15ê°œ ë™ì‹œ ìš”ì²­
        # í”„ë¡œë¹„ì €ë‹ëœ ìš©ëŸ‰ì´ ì—†ìœ¼ë©´ (0) ON_DEMAND ëª¨ë¸ ì‚¬ìš©
        if provisioned_units > 0:
            max_concurrent = provisioned_units * 15
            logger.info(
                f"ğŸ“Š í”„ë¡œë¹„ì €ë‹ëœ ìš©ëŸ‰: {provisioned_units} MU â†’ "
                f"ë™ì‹œì„± ì œí•œ: {max_concurrent}ê°œ ë™ì‹œ ìš”ì²­"
            )
        else:
            # ON_DEMAND ëª¨ë¸: 10ê°œ ë™ì‹œ ìš”ì²­ ì œí•œ
            # - Rate Limit ë³´í˜¸
            # - $300/ì›” ì˜ˆì‚° ê¸°ì¤€ ì•ˆì •ì  ìš´ì˜ (ì¼í‰ê·  950íšŒ ìš”ì²­ ì²˜ë¦¬ ê°€ëŠ¥)
            # - 100ëª… ë™ì‹œ ì ‘ì† ê°€ëŠ¥ (ìš”ì²­ì€ 10ê°œì”© ìˆœì°¨ ì²˜ë¦¬)
            max_concurrent = 10
            logger.info(
                f"ğŸ“Š ON_DEMAND ëª¨ë¸ ì‚¬ìš© â†’ ë™ì‹œì„± ì œí•œ: {max_concurrent}ê°œ ë™ì‹œ ìš”ì²­ "
                f"(Rate Limit ë³´í˜¸, ì˜ˆì‚°: $300/ì›” ê¸°ì¤€)"
            )
        
        # ThreadPoolExecutor ì´ˆê¸°í™” (ìµœì´ˆ 1íšŒë§Œ)
        if BedrockClient._executor is None:
            # ìŠ¤ë ˆë“œ í’€ í¬ê¸°: ë™ì‹œì„± ì œí•œì˜ 1.5ë°° (ì—¬ìœ ë¶„ í™•ë³´)
            thread_pool_size = max(max_concurrent * 2, 20)
            BedrockClient._executor = ThreadPoolExecutor(
                max_workers=thread_pool_size,
                thread_name_prefix="bedrock-llm"
            )
            logger.info(f"âœ… Bedrock ThreadPoolExecutor ì´ˆê¸°í™” ì™„ë£Œ (max_workers={thread_pool_size})")
        
        # Semaphore ì´ˆê¸°í™” (ìµœì´ˆ 1íšŒë§Œ ë˜ëŠ” í”„ë¡œë¹„ì €ë‹ëœ ìš©ëŸ‰ ë³€ê²½ ì‹œ)
        if BedrockClient._semaphore is None or BedrockClient._max_concurrent_requests != max_concurrent:
            BedrockClient._max_concurrent_requests = max_concurrent
            BedrockClient._provisioned_model_units = provisioned_units
            BedrockClient._semaphore = asyncio.Semaphore(max_concurrent)
            logger.info(
                f"âœ… Bedrock ë™ì‹œì„± ì œí•œ ì„¤ì • ì™„ë£Œ "
                f"(í”„ë¡œë¹„ì €ë‹: {provisioned_units} MU, "
                f"ë™ì‹œ ìš”ì²­: {max_concurrent}ê°œ)"
            )
        
        logger.info(f"Bedrock Client ì´ˆê¸°í™”: ëª¨ë¸={self.model}, ë¦¬ì „={config.region_name}")

    def _convert_messages(
        self, messages: List[Dict[str, str]]
    ) -> tuple[Optional[str], List[Dict[str, str]]]:
        """
        OpenAI í˜•ì‹ ë©”ì‹œì§€ë¥¼ Bedrock (Anthropic) í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        """
        system_message = None
        converted_messages = []

        for msg in messages:
            if msg.get("role") == "system":
                system_message = msg.get("content")
            else:
                converted_messages.append(msg)

        if system_message is None:
            system_message = self.system_prompt

        return system_message, converted_messages

    async def generate(
        self,
        messages: List[Dict[str, str]],
        temperature: float = 0.7,
        max_tokens: int = 4000,
        **kwargs
    ) -> str:
        """ë¹„ë™ê¸° ì™„ë£Œ ìƒì„±"""
        try:
            # ë©”ì‹œì§€ í˜•ì‹ ë³€í™˜
            system_message, converted_messages = self._convert_messages(messages)

            # ëŸ°íƒ€ì„ ëª¨ë¸ ì˜¤ë²„ë¼ì´ë“œ ì§€ì›
            model_id = kwargs.pop("model", None) or self.model

            # Bedrock API ìš”ì²­ ë³¸ë¬¸
            body = {
                "anthropic_version": "bedrock-2023-05-31",
                "max_tokens": max_tokens,
                "temperature": temperature,
                "messages": converted_messages
            }

            # system í”„ë¡¬í”„íŠ¸ ì¶”ê°€
            if system_message:
                body["system"] = system_message

            # ë™ì‹œì„± ì œí•œ: Semaphoreë¡œ ë™ì‹œ ìš”ì²­ ìˆ˜ ì œì–´
            await LLMRateLimiter.acquire("bedrock")
            async with BedrockClient._semaphore:
                # Bedrock API í˜¸ì¶œ (ë™ê¸° ë°©ì‹ - boto3ëŠ” async ë¯¸ì§€ì›)
                # ThreadPoolExecutorë¥¼ ì‚¬ìš©í•˜ì—¬ ë…¼ë¸”ë¡œí‚¹ ì²˜ë¦¬
                loop = asyncio.get_event_loop()
                response = await loop.run_in_executor(
                    BedrockClient._executor,
                    lambda: self.client.invoke_model(
                        modelId=model_id,
                        body=json.dumps(body)
                    )
                )

            # ì‘ë‹µ íŒŒì‹±
            response_body = json.loads(response['body'].read())

            # í† í° ì‚¬ìš©ëŸ‰ ì¶”ì¶œ ë° ì €ì¥
            usage = response_body.get('usage', {})
            input_tokens = usage.get('input_tokens', 0)
            output_tokens = usage.get('output_tokens', 0)

            # í† í° ì‚¬ìš©ëŸ‰ ë©”íƒ€ë°ì´í„° ì €ì¥ (middlewareì—ì„œ ì ‘ê·¼ ê°€ëŠ¥)
            self.last_usage = {
                'input_tokens': input_tokens,
                'output_tokens': output_tokens,
                'total_tokens': input_tokens + output_tokens,
                'cache_read_tokens': usage.get('cache_read_input_token_count', 0),
                'cache_write_tokens': usage.get('cache_creation_input_token_count', 0),
                'model': model_id
            }

            logger.info(
                f"Bedrock í† í° ì‚¬ìš©ëŸ‰ - ì…ë ¥: {input_tokens}, ì¶œë ¥: {output_tokens}, ì´: {input_tokens + output_tokens}"
            )

            return response_body['content'][0]['text']

        except ClientError as e:
            error_code = e.response.get('Error', {}).get('Code', '')
            error_message = e.response.get('Error', {}).get('Message', str(e))

            if error_code == 'ThrottlingException':
                logger.error(f"Bedrock API ì‚¬ìš©ëŸ‰ ì œí•œ: {error_message}")
                raise LLMRateLimitError(
                    message="Bedrock API ì‚¬ìš©ëŸ‰ ì œí•œì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤",
                    details={"model": model_id, "error": error_message}
                )
            elif "on-demand throughput isn't supported" in error_message.lower():
                # ON_DEMANDë¥¼ ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ (INFERENCE_PROFILEë§Œ ì§€ì›)
                logger.error(
                    f"Bedrock ëª¨ë¸ì´ ON_DEMANDë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŒ: {model_id}. "
                    f"ì´ ëª¨ë¸ì€ í”„ë¡œë¹„ì €ë‹ëœ ìš©ëŸ‰(Provisioned Throughput)ì´ í•„ìš”í•©ë‹ˆë‹¤."
                )
                raise LLMAPIError(
                    message=(
                        f"ì„ íƒí•œ ëª¨ë¸ '{model_id}'ì€(ëŠ”) ON_DEMAND ëª¨ë“œë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. "
                        f"ì´ ëª¨ë¸ì€ í”„ë¡œë¹„ì €ë‹ëœ ìš©ëŸ‰(Provisioned Throughput)ì´ í•„ìš”í•©ë‹ˆë‹¤. "
                        f"ON_DEMANDë¥¼ ì§€ì›í•˜ëŠ” ëª¨ë¸(ì˜ˆ: Claude 3 Haiku, Claude 3.5 Sonnet)ì„ ì„ íƒí•´ì£¼ì„¸ìš”."
                    ),
                    details={
                        "model": model_id,
                        "error_code": error_code,
                        "error": error_message,
                        "requires_provisioned_throughput": True
                    }
                )
            elif error_code == "INVALID_PAYMENT_INSTRUMENT" or "payment instrument" in error_message.lower():
                # ê²°ì œìˆ˜ë‹¨/ëª¨ë¸ êµ¬ë… ë¯¸ì™„ë£Œ â†’ ê¸°ë³¸ ëª¨ë¸ë¡œ ìë™ í´ë°± ì‹œë„
                if model_id != self.model:
                    logger.warning(
                        "Bedrock ëª¨ë¸ %s ì ‘ê·¼ ê±°ë¶€(INVALID_PAYMENT_INSTRUMENT). ê¸°ë³¸ ëª¨ë¸(%s)ë¡œ í´ë°± ì‹œë„.",
                        model_id,
                        self.model
                    )
                    return await self.generate(
                        messages=messages,
                        temperature=temperature,
                        max_tokens=max_tokens,
                        **kwargs
                    )

                logger.error(
                    "Bedrock ëª¨ë¸ ì ‘ê·¼ ê±°ë¶€(INVALID_PAYMENT_INSTRUMENT): %s. "
                    "ê²°ì œ ìˆ˜ë‹¨/ëª¨ë¸ ì•¡ì„¸ìŠ¤ ìŠ¹ì¸ í•„ìš”.",
                    error_message
                )
                raise LLMAPIError(
                    message=(
                        "Bedrock ëª¨ë¸ ê²°ì œ/ì ‘ê·¼ì´ í™œì„±í™”ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. "
                        "AWS ì½˜ì†”ì—ì„œ ê²°ì œ ìˆ˜ë‹¨ ë“±ë¡ ë˜ëŠ” í•´ë‹¹ ëª¨ë¸ ì•¡ì„¸ìŠ¤ ìŠ¹ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤."
                    ),
                    details={"model": model_id, "error_code": error_code, "error": error_message}
                )
            else:
                logger.error(f"Bedrock API ì˜¤ë¥˜: {error_message}")
                raise LLMAPIError(
                    message=f"Bedrock API í˜¸ì¶œ ì‹¤íŒ¨: {error_message}",
                    details={"model": model_id, "error_code": error_code}
                )

        except BotoCoreError as e:
            logger.error(f"Bedrock ì—°ê²° ì˜¤ë¥˜: {e}")
            raise LLMAPIError(
                message=f"Bedrock ì—°ê²° ì‹¤íŒ¨: {str(e)}",
                details={"model": self.model}
            )

        except Exception as e:
            logger.error(f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
            raise LLMAPIError(
                message=f"LLM ìƒì„± ì‹¤íŒ¨: {str(e)}",
                details={"model": self.model}
            )

    async def generate_stream(
        self,
        messages: List[Dict[str, str]],
        temperature: float = 0.7,
        max_tokens: int = 4000,
        **kwargs
    ) -> AsyncGenerator[str, None]:
        """ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±"""
        try:
            # ë©”ì‹œì§€ í˜•ì‹ ë³€í™˜
            system_message, converted_messages = self._convert_messages(messages)

            # ëŸ°íƒ€ì„ ëª¨ë¸ ì˜¤ë²„ë¼ì´ë“œ ì§€ì›
            model_id = kwargs.pop("model", None) or self.model

            # Bedrock API ìš”ì²­ ë³¸ë¬¸
            body = {
                "anthropic_version": "bedrock-2023-05-31",
                "max_tokens": max_tokens,
                "temperature": temperature,
                "messages": converted_messages
            }

            if system_message:
                body["system"] = system_message

            # ë™ì‹œì„± ì œí•œ: Semaphoreë¡œ ë™ì‹œ ìš”ì²­ ìˆ˜ ì œì–´
            await LLMRateLimiter.acquire("bedrock")
            async with BedrockClient._semaphore:
                # Bedrock ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ (ë™ê¸° ë°©ì‹ - boto3ëŠ” async ë¯¸ì§€ì›)
                # ThreadPoolExecutorë¥¼ ì‚¬ìš©í•˜ì—¬ ë…¼ë¸”ë¡œí‚¹ ì²˜ë¦¬
                loop = asyncio.get_event_loop()
                response = await loop.run_in_executor(
                    BedrockClient._executor,
                    lambda: self.client.invoke_model_with_response_stream(
                        modelId=model_id,
                        body=json.dumps(body)
                    )
                )

            # ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬
            stream = response.get('body')
            total_input_tokens = 0
            total_output_tokens = 0
            cache_read_tokens = 0
            cache_write_tokens = 0

            def _update_cache_usage(usage_data: Optional[Dict[str, int]]) -> None:
                """Bedrock ìŠ¤íŠ¸ë¦¬ë° ì´ë²¤íŠ¸ì—ì„œ ìºì‹œ í† í° ìˆ˜ì¹˜ë¥¼ ì¶”ì¶œ"""
                nonlocal cache_read_tokens, cache_write_tokens
                if not usage_data:
                    return

                def _extract_value(*keys: str) -> int:
                    for key in keys:
                        if key in usage_data and usage_data[key] is not None:
                            return int(usage_data[key])
                    return 0

                read_tokens = _extract_value(
                    'cache_read_input_token_count',
                    'cache_read_input_tokens',
                    'cache_read_tokens'
                )
                write_tokens = _extract_value(
                    'cache_creation_input_token_count',
                    'cache_creation_input_tokens',
                    'cache_write_tokens'
                )

                # Bedrockì€ ëˆ„ì  ìˆ˜ì¹˜ë¥¼ ë³´ë‚´ë¯€ë¡œ ê°€ì¥ í° ê°’ì„ ìœ ì§€
                if read_tokens > cache_read_tokens:
                    cache_read_tokens = read_tokens
                if write_tokens > cache_write_tokens:
                    cache_write_tokens = write_tokens

            if stream:
                for event in stream:
                    chunk = event.get('chunk')
                    if chunk:
                        chunk_data = json.loads(chunk.get('bytes').decode())

                        # content_block_delta ì´ë²¤íŠ¸ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                        if chunk_data.get('type') == 'content_block_delta':
                            delta = chunk_data.get('delta', {})
                            if delta.get('type') == 'text_delta':
                                text = delta.get('text', '')
                                if text:
                                    yield text

                        # message_deltaì—ì„œ í† í° ì‚¬ìš©ëŸ‰ ì¶”ì¶œ
                        elif chunk_data.get('type') == 'message_delta':
                            usage = chunk_data.get('usage', {})
                            total_output_tokens = usage.get('output_tokens', total_output_tokens)
                            _update_cache_usage(usage)

                        # message_startì—ì„œ ì…ë ¥ í† í° ì¶”ì¶œ
                        elif chunk_data.get('type') == 'message_start':
                            usage = chunk_data.get('message', {}).get('usage', {})
                            total_input_tokens = usage.get('input_tokens', 0)
                            _update_cache_usage(usage)

                        elif chunk_data.get('type') == 'message_stop':
                            usage = chunk_data.get('usage', {})
                            total_output_tokens = usage.get('output_tokens', total_output_tokens)
                            _update_cache_usage(usage)

            # ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ í›„ í† í° ì‚¬ìš©ëŸ‰ ì €ì¥
            self.last_usage = {
                'input_tokens': total_input_tokens,
                'output_tokens': total_output_tokens,
                'total_tokens': total_input_tokens + total_output_tokens,
                'cache_read_tokens': cache_read_tokens,
                'cache_write_tokens': cache_write_tokens,
                'model': model_id
            }

            logger.info(
                f"Bedrock ìŠ¤íŠ¸ë¦¬ë° í† í° ì‚¬ìš©ëŸ‰ - ì…ë ¥: {total_input_tokens}, ì¶œë ¥: {total_output_tokens}"
            )

        except ClientError as e:
            error_code = e.response.get('Error', {}).get('Code', '')
            error_message = e.response.get('Error', {}).get('Message', str(e))

            if error_code == 'ThrottlingException':
                logger.error(f"Bedrock API ì‚¬ìš©ëŸ‰ ì œí•œ: {error_message}")
                raise LLMRateLimitError(
                    message="Bedrock API ì‚¬ìš©ëŸ‰ ì œí•œì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤",
                    details={"model": model_id, "error": error_message}
                )
            elif "on-demand throughput isn't supported" in error_message.lower():
                # ON_DEMANDë¥¼ ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ (INFERENCE_PROFILEë§Œ ì§€ì›)
                logger.error(
                    f"Bedrock ëª¨ë¸ì´ ON_DEMANDë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŒ: {model_id}. "
                    f"ì´ ëª¨ë¸ì€ í”„ë¡œë¹„ì €ë‹ëœ ìš©ëŸ‰(Provisioned Throughput)ì´ í•„ìš”í•©ë‹ˆë‹¤."
                )
                raise LLMAPIError(
                    message=(
                        f"ì„ íƒí•œ ëª¨ë¸ '{model_id}'ì€(ëŠ”) ON_DEMAND ëª¨ë“œë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. "
                        f"ì´ ëª¨ë¸ì€ í”„ë¡œë¹„ì €ë‹ëœ ìš©ëŸ‰(Provisioned Throughput)ì´ í•„ìš”í•©ë‹ˆë‹¤. "
                        f"ON_DEMANDë¥¼ ì§€ì›í•˜ëŠ” ëª¨ë¸(ì˜ˆ: Claude 3 Haiku, Claude 3.5 Sonnet)ì„ ì„ íƒí•´ì£¼ì„¸ìš”."
                    ),
                    details={
                        "model": model_id,
                        "error_code": error_code,
                        "error": error_message,
                        "requires_provisioned_throughput": True
                    }
                )
            elif error_code == "INVALID_PAYMENT_INSTRUMENT" or "payment instrument" in error_message.lower():
                if model_id != self.model:
                    logger.warning(
                        "Bedrock ëª¨ë¸ %s ì ‘ê·¼ ê±°ë¶€(INVALID_PAYMENT_INSTRUMENT). ê¸°ë³¸ ëª¨ë¸(%s)ë¡œ ìŠ¤íŠ¸ë¦¬ë° í´ë°± ì‹œë„.",
                        model_id,
                        self.model
                    )
                    async for chunk in self.generate_stream(
                        messages=messages,
                        temperature=temperature,
                        max_tokens=max_tokens,
                        model=self.model,
                        **kwargs
                    ):
                        yield chunk
                    return

                logger.error(
                    "Bedrock ëª¨ë¸ ì ‘ê·¼ ê±°ë¶€(INVALID_PAYMENT_INSTRUMENT): %s. "
                    "ê²°ì œ ìˆ˜ë‹¨/ëª¨ë¸ ì•¡ì„¸ìŠ¤ ìŠ¹ì¸ í•„ìš”.",
                    error_message
                )
                raise LLMAPIError(
                    message=(
                        "Bedrock ëª¨ë¸ ê²°ì œ/ì ‘ê·¼ì´ í™œì„±í™”ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. "
                        "AWS ì½˜ì†”ì—ì„œ ê²°ì œ ìˆ˜ë‹¨ ë“±ë¡ ë˜ëŠ” í•´ë‹¹ ëª¨ë¸ ì•¡ì„¸ìŠ¤ ìŠ¹ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤."
                    ),
                    details={"model": model_id, "error_code": error_code, "error": error_message}
                )
            else:
                logger.error(f"Bedrock API ì˜¤ë¥˜: {error_message}")
                raise LLMAPIError(
                    message=f"Bedrock API í˜¸ì¶œ ì‹¤íŒ¨: {error_message}",
                    details={"model": model_id, "error_code": error_code}
                )

        except Exception as e:
            logger.error(f"ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {e}")
            raise LLMAPIError(
                message=f"ìŠ¤íŠ¸ë¦¬ë° ìƒì„± ì‹¤íŒ¨: {str(e)}",
                details={"model": self.model}
            )
