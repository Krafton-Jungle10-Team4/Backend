"""
ìž„ë² ë”© ì›Œì»¤ ì„œë¹„ìŠ¤

SQS íì—ì„œ ë¬¸ì„œ ì²˜ë¦¬ ë©”ì‹œì§€ë¥¼ ìˆ˜ì‹ í•˜ê³  ë°±ê·¸ë¼ìš´ë“œì—ì„œ ìž„ë² ë”© ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
"""
import asyncio
import json
import os
import time
import traceback
from datetime import datetime, timezone
from pathlib import Path
from typing import Optional, Dict, Any

from sqlalchemy import select, update
from sqlalchemy.ext.asyncio import AsyncSession, create_async_engine, async_sessionmaker

from app.config import settings
from app.core.logging_config import get_logger
from app.core.aws_clients import get_s3_client, get_sqs_client
from app.models.document import Document, DocumentStatus
from app.core.embeddings import get_embedding_service, CircuitBreakerOpenError
from app.core.vector_store import get_vector_store
from app.core.document_processor import DocumentProcessor
from app.core.chunking import get_text_chunker
from app.core.exceptions import (
    DocumentProcessingError,
    DocumentParsingError,
    VectorStoreError
)

logger = get_logger(__name__)


class EmbeddingWorker:
    """
    ìž„ë² ë”© ì›Œì»¤ ì„œë¹„ìŠ¤

    SQS íì—ì„œ ë¬¸ì„œ ì²˜ë¦¬ ìž‘ì—…ì„ í´ë§í•˜ê³  S3ì—ì„œ íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí•˜ì—¬
    íŒŒì‹± â†’ ì²­í‚¹ â†’ ìž„ë² ë”© â†’ pgvector ì €ìž¥ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
    """

    def __init__(self):
        self.s3_client = get_s3_client()
        self.sqs_client = get_sqs_client()
        self.embedding_service = get_embedding_service()
        self.document_processor = DocumentProcessor()
        self.text_chunker = get_text_chunker()

        # ìž„ì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
        Path(settings.upload_temp_dir).mkdir(parents=True, exist_ok=True)

        # ë¹„ë™ê¸° DB ì„¸ì…˜ ìƒì„±
        self.engine = create_async_engine(
            settings.get_database_url(),
            echo=False,
            pool_pre_ping=True,
            pool_size=5,
            max_overflow=10
        )
        self.async_session = async_sessionmaker(
            self.engine,
            class_=AsyncSession,
            expire_on_commit=False
        )

        logger.info("ìž„ë² ë”© ì›Œì»¤ ì´ˆê¸°í™” ì™„ë£Œ")

    async def start(self):
        """ì›Œì»¤ ì‹œìž‘ (ë¬´í•œ ë£¨í”„)"""
        logger.info("ðŸš€ ìž„ë² ë”© ì›Œì»¤ ì‹œìž‘")
        logger.info(f"SQS í: {settings.sqs_queue_url}")
        logger.info(f"S3 ë²„í‚·: {settings.s3_bucket_name}")
        logger.info(f"Long Polling: 5ì´ˆ")

        while True:
            try:
                # SQSì—ì„œ ë©”ì‹œì§€ ìˆ˜ì‹  (Long Polling)
                messages = await self.sqs_client.receive_messages(
                    max_messages=1,
                    wait_time_seconds=5  # Long Polling (ë¹ ë¥¸ ì‘ë‹µì„± í™•ë³´)
                )

                if not messages:
                    logger.debug("ìˆ˜ì‹ ëœ ë©”ì‹œì§€ ì—†ìŒ")
                    continue

                # ë©”ì‹œì§€ ì²˜ë¦¬
                for message in messages:
                    await self._process_message(message)

            except KeyboardInterrupt:
                logger.info("ì›Œì»¤ ì¢…ë£Œ ì‹ í˜¸ ìˆ˜ì‹ ")
                break
            except Exception as e:
                logger.error(f"ì›Œì»¤ ë©”ì¸ ë£¨í”„ ì—ëŸ¬: {e}", exc_info=True)
                await asyncio.sleep(5)  # ì—ëŸ¬ ë°œìƒ ì‹œ 5ì´ˆ ëŒ€ê¸°

    async def _process_message(self, message: Dict[str, Any]):
        """
        SQS ë©”ì‹œì§€ ì²˜ë¦¬

        Args:
            message: SQS ë©”ì‹œì§€ ê°ì²´
        """
        receipt_handle = message.get("ReceiptHandle")
        message_id = message.get("MessageId")

        try:
            # ë©”ì‹œì§€ ë³¸ë¬¸ íŒŒì‹±
            body = json.loads(message.get("Body", "{}"))
            document_id = body.get("document_id")
            bot_id = body.get("bot_id")
            user_uuid = body.get("user_uuid")
            s3_uri = body.get("s3_uri")
            original_filename = body.get("original_filename")
            file_extension = body.get("file_extension")
            retry_count = body.get("retry_count", 0)

            logger.info(f"ðŸ“¨ ë©”ì‹œì§€ ìˆ˜ì‹ : document_id={document_id}, file={original_filename}, retry={retry_count}")

            # í•„ìˆ˜ í•„ë“œ ê²€ì¦
            if not all([document_id, bot_id, s3_uri, original_filename]):
                logger.error(f"ë©”ì‹œì§€ í•„ë“œ ëˆ„ë½: {body}")
                await self.sqs_client.delete_message(receipt_handle)
                return

            # ë¬¸ì„œ ì²˜ë¦¬
            start_time = time.time()
            await self._process_document(
                document_id=document_id,
                bot_id=bot_id,
                user_uuid=user_uuid,
                s3_uri=s3_uri,
                original_filename=original_filename,
                file_extension=file_extension
            )

            processing_time = int(time.time() - start_time)
            logger.info(f"âœ… ë¬¸ì„œ ì²˜ë¦¬ ì™„ë£Œ: document_id={document_id} ({processing_time}ì´ˆ)")

            # ë©”ì‹œì§€ ì‚­ì œ (ì²˜ë¦¬ ì™„ë£Œ)
            await self.sqs_client.delete_message(receipt_handle)

        except Exception as e:
            logger.error(f"âŒ ë©”ì‹œì§€ ì²˜ë¦¬ ì‹¤íŒ¨ (message_id={message_id}): {e}", exc_info=True)
            # ë©”ì‹œì§€ë¥¼ ì‚­ì œí•˜ì§€ ì•Šìœ¼ë©´ ìžë™ìœ¼ë¡œ DLQë¡œ ì´ë™ (maxReceiveCount ì´ˆê³¼ ì‹œ)

    async def _process_document(
        self,
        document_id: str,
        bot_id: str,
        user_uuid: str,
        s3_uri: str,
        original_filename: str,
        file_extension: str
    ):
        """
        ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸

        1. ìƒíƒœë¥¼ PROCESSINGìœ¼ë¡œ ë³€ê²½
        2. S3ì—ì„œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
        3. íŒŒì‹± â†’ ì²­í‚¹ â†’ ìž„ë² ë”© â†’ pgvector ì €ìž¥
        4. ìƒíƒœë¥¼ DONEìœ¼ë¡œ ë³€ê²½
        """
        async with self.async_session() as db:
            try:
                # 1. ìƒíƒœë¥¼ PROCESSINGìœ¼ë¡œ ë³€ê²½
                await self._update_document_status(
                    db=db,
                    document_id=document_id,
                    status=DocumentStatus.PROCESSING,
                    processing_started_at=datetime.now(timezone.utc)
                )

                # 2. S3ì—ì„œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
                logger.info(f"S3 ë‹¤ìš´ë¡œë“œ ì‹œìž‘: {s3_uri}")
                s3_key = s3_uri.replace(f"s3://{settings.s3_bucket_name}/", "")
                file_content = await self.s3_client.download_file(s3_key)

                # 3. ìž„ì‹œ íŒŒì¼ë¡œ ì €ìž¥
                temp_file_path = os.path.join(
                    settings.upload_temp_dir,
                    f"{document_id}{Path(original_filename).suffix}"
                )
                with open(temp_file_path, "wb") as f:
                    f.write(file_content)

                try:
                    # 4. ë¬¸ì„œ íŒŒì‹±
                    logger.info(f"ë¬¸ì„œ íŒŒì‹± ì‹œìž‘: {original_filename}")
                    text = self.document_processor.process_file(temp_file_path)

                    if not text or not text.strip():
                        raise DocumentParsingError("ë¬¸ì„œì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

                    # 5. í…ìŠ¤íŠ¸ ì²­í‚¹
                    logger.info(f"í…ìŠ¤íŠ¸ ì²­í‚¹ ì‹œìž‘")
                    chunks = self.text_chunker.split_text(text)

                    if not chunks:
                        raise DocumentProcessingError("í…ìŠ¤íŠ¸ ì²­í‚¹ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤")

                    # 6. ìž„ë² ë”© ìƒì„±
                    logger.info(f"ìž„ë² ë”© ìƒì„± ì‹œìž‘: {len(chunks)}ê°œ ì²­í¬")
                    try:
                        embeddings = await self.embedding_service.embed_documents(chunks)
                    except CircuitBreakerOpenError as e:
                        # Circuit Breakerê°€ ì—´ë¦° ê²½ìš°: ë©”ì‹œì§€ë¥¼ ë‹¤ì‹œ íë¡œ ë°˜í™˜ (ìž¬ì‹œë„)
                        logger.warning(f"Circuit Breaker ì—´ë¦¼: {e}")
                        await self._update_document_status(
                            db=db,
                            document_id=document_id,
                            status=DocumentStatus.PENDING,
                            error_message=f"Circuit Breaker ìž‘ë™ - ìž¬ì‹œë„ ëŒ€ê¸° ì¤‘"
                        )
                        # ë©”ì‹œì§€ë¥¼ ì‚­ì œí•˜ì§€ ì•Šìœ¼ë©´ ìžë™ìœ¼ë¡œ ìž¬ì‹œë„ë¨
                        raise

                    # 7. ë©”íƒ€ë°ì´í„° ìƒì„±
                    file_size = os.path.getsize(temp_file_path)
                    metadata = self.document_processor.extract_metadata(temp_file_path, file_size)
                    metadata.update({
                        "document_id": document_id,
                        "bot_id": bot_id,
                        "user_uuid": user_uuid,
                        "original_filename": original_filename,
                        "created_at": datetime.now().isoformat(),
                        "chunk_count": len(chunks)
                    })

                    # 8. ë²¡í„° ìŠ¤í† ì–´ì— ì €ìž¥
                    logger.info(f"ë²¡í„° ìŠ¤í† ì–´ì— ì €ìž¥ ì‹œìž‘ (bot_id={bot_id})")
                    vector_store = get_vector_store(bot_id=bot_id, user_uuid=user_uuid, db=db)

                    chunk_ids = [f"{document_id}_chunk_{i}" for i in range(len(chunks))]
                    chunk_metadatas = [
                        {
                            **metadata,
                            "chunk_index": i,
                            "chunk_id": chunk_ids[i]
                        }
                        for i in range(len(chunks))
                    ]

                    await vector_store.add_documents(
                        ids=chunk_ids,
                        embeddings=embeddings,
                        documents=chunks,
                        metadatas=chunk_metadatas,
                        source_document_id=document_id  # â† documents í…Œì´ë¸” ì—°ê²°
                    )

                    # 9. ìƒíƒœë¥¼ DONEìœ¼ë¡œ ë³€ê²½
                    await self._update_document_status(
                        db=db,
                        document_id=document_id,
                        status=DocumentStatus.DONE,
                        chunk_count=len(chunks),
                        completed_at=datetime.now(timezone.utc)
                    )

                    logger.info(f"âœ… ë¬¸ì„œ ì²˜ë¦¬ ì„±ê³µ: {document_id} ({len(chunks)} ì²­í¬)")

                finally:
                    # 10. ìž„ì‹œ íŒŒì¼ ì‚­ì œ
                    self._cleanup_temp_file(temp_file_path)

            except DocumentParsingError as e:
                logger.error(f"ë¬¸ì„œ íŒŒì‹± ì‹¤íŒ¨: {e}")
                await self._update_document_status(
                    db=db,
                    document_id=document_id,
                    status=DocumentStatus.FAILED,
                    error_message=f"ë¬¸ì„œ íŒŒì‹± ì‹¤íŒ¨: {str(e)}",
                    completed_at=datetime.now(timezone.utc)
                )
                raise

            except VectorStoreError as e:
                logger.error(f"ë²¡í„° ì €ìž¥ ì‹¤íŒ¨: {e}")
                await self._update_document_status(
                    db=db,
                    document_id=document_id,
                    status=DocumentStatus.FAILED,
                    error_message=f"ë²¡í„° ì €ìž¥ ì‹¤íŒ¨: {str(e)}",
                    completed_at=datetime.now(timezone.utc)
                )
                raise

            except Exception as e:
                logger.error(f"ë¬¸ì„œ ì²˜ë¦¬ ì‹¤íŒ¨: {e}", exc_info=True)
                error_trace = traceback.format_exc()
                await self._update_document_status(
                    db=db,
                    document_id=document_id,
                    status=DocumentStatus.FAILED,
                    error_message=f"{type(e).__name__}: {str(e)}",
                    completed_at=datetime.now(timezone.utc)
                )
                raise

    async def _update_document_status(
        self,
        db: AsyncSession,
        document_id: str,
        status: DocumentStatus,
        error_message: Optional[str] = None,
        chunk_count: Optional[int] = None,
        processing_started_at: Optional[datetime] = None,
        completed_at: Optional[datetime] = None
    ):
        """
        documents í…Œì´ë¸” ìƒíƒœ ì—…ë°ì´íŠ¸

        Args:
            db: ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜
            document_id: ë¬¸ì„œ ID
            status: ìƒˆ ìƒíƒœ
            error_message: ì—ëŸ¬ ë©”ì‹œì§€ (ì‹¤íŒ¨ ì‹œ)
            chunk_count: ì²­í¬ ê°œìˆ˜ (ì™„ë£Œ ì‹œ)
            processing_started_at: ì²˜ë¦¬ ì‹œìž‘ ì‹œê°„
            completed_at: ì™„ë£Œ ì‹œê°„
        """
        try:
            # Document ì¡°íšŒ
            result = await db.execute(
                select(Document).where(Document.document_id == document_id)
            )
            document = result.scalar_one_or_none()

            if not document:
                logger.error(f"ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {document_id}")
                return

            # ìƒíƒœ ì—…ë°ì´íŠ¸
            document.status = status
            document.updated_at = datetime.now(timezone.utc)

            if error_message:
                document.error_message = error_message

            if chunk_count is not None:
                document.chunk_count = chunk_count

            if processing_started_at:
                document.processing_started_at = processing_started_at

            if completed_at:
                document.completed_at = completed_at
                # ì²˜ë¦¬ ì‹œê°„ ê³„ì‚° (ì´ˆ)
                if document.processing_started_at:
                    # timezone-naive datetimeì„ timezone-awareë¡œ ë³€í™˜
                    start_time = document.processing_started_at
                    if start_time.tzinfo is None:
                        start_time = start_time.replace(tzinfo=timezone.utc)
                    processing_time = (completed_at - start_time).total_seconds()
                    document.processing_time = int(processing_time)

                # ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œëœ ê²½ìš° embedded_at ì„¤ì •
                if status == DocumentStatus.DONE:
                    document.embedded_at = completed_at

            await db.commit()
            logger.info(f"ìƒíƒœ ì—…ë°ì´íŠ¸: document_id={document_id}, status={status.value}")

        except Exception as e:
            logger.error(f"ìƒíƒœ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}", exc_info=True)
            await db.rollback()
            raise

    def _cleanup_temp_file(self, file_path: str):
        """ìž„ì‹œ íŒŒì¼ ì‚­ì œ"""
        try:
            if os.path.exists(file_path):
                os.remove(file_path)
                logger.debug(f"ìž„ì‹œ íŒŒì¼ ì‚­ì œ: {file_path}")
        except Exception as e:
            logger.warning(f"ìž„ì‹œ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {file_path}, {e}")

    async def shutdown(self):
        """ì›Œì»¤ ì¢…ë£Œ ì‹œ ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        logger.info("ì›Œì»¤ ì¢…ë£Œ ì¤‘...")
        await self.engine.dispose()
        logger.info("ì›Œì»¤ ì¢…ë£Œ ì™„ë£Œ")


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
_worker_instance: Optional[EmbeddingWorker] = None


def get_embedding_worker() -> EmbeddingWorker:
    """ìž„ë² ë”© ì›Œì»¤ ì‹±ê¸€í†¤"""
    global _worker_instance
    if _worker_instance is None:
        _worker_instance = EmbeddingWorker()
    return _worker_instance
